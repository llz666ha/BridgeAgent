#!/usr/bin/env python3
"""
BNERC Server - åŸºäº LangGraph çš„ Plan & Execute Agent ç”Ÿäº§æœåŠ¡å™¨
ç¬¦åˆ ASGI è§„èŒƒ
"""

import json
import os
import sys
import re
import logging
import asyncio
import uuid
from contextlib import asynccontextmanager
from typing import Dict, List, Any, Optional
from logging.handlers import RotatingFileHandler

from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ================================
# æ—¥å¿—é…ç½®
# ================================
def setup_logging():
    """è®¾ç½®ç”Ÿäº§ç¯å¢ƒæ—¥å¿—é…ç½®"""
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    
    # ç§»é™¤é»˜è®¤å¤„ç†å™¨
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # æ ¼å¼åŒ–å™¨
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # æ–‡ä»¶å¤„ç†å™¨ï¼ˆå¸¦è½®è½¬ï¼‰- ä½¿ç”¨ç»å¯¹è·¯å¾„
    log_dir = os.path.dirname(os.path.abspath(__file__))
    log_file = os.path.join(log_dir, "bnerc_server.log")
    file_handler = RotatingFileHandler(
        log_file,
        maxBytes=10*1024*1024,  # 10MB
        backupCount=5,
        encoding='utf-8'
    )
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    
    # é™ä½ç¬¬ä¸‰æ–¹åº“æ—¥å¿—çº§åˆ«
    logging.getLogger("uvicorn").setLevel(logging.WARNING)
    logging.getLogger("fastapi").setLevel(logging.WARNING)
    logging.getLogger("httpx").setLevel(logging.WARNING)
    
    return logger

logger = setup_logging()

# ================================
# LangGraph ç›¸å…³å¯¼å…¥
# ================================
from langchain_core.messages import AIMessage, SystemMessage, HumanMessage, ToolMessage
from langgraph.prebuilt import ToolNode
from langgraph.graph import StateGraph, END, MessagesState
from langgraph.types import StreamWriter
# PostgreSQL checkpointer æ”¯æŒ
POSTGRES_AVAILABLE = False
PostgresSaver = None
AsyncPostgresSaver = None
AsyncConnectionPool = None

try:
    # å°è¯•å¯¼å…¥ PostgreSQL checkpointerï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼Œæ¨èï¼‰
    from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
    # å°è¯•å¯¼å…¥è¿æ¥æ± 
    try:
        from psycopg_pool import AsyncConnectionPool
        logger.info("psycopg_pool å¯ç”¨ï¼Œå°†ä½¿ç”¨è¿æ¥æ± ")
    except ImportError:
        logger.warning("psycopg_pool ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ç›´æ¥è¿æ¥æ–¹å¼")
    POSTGRES_AVAILABLE = True
    logger.info("PostgreSQL checkpointer å¯ç”¨ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰")
except ImportError:
    try:
        # å°è¯•åŒæ­¥ç‰ˆæœ¬
        from langgraph.checkpoint.postgres import PostgresSaver
        POSTGRES_AVAILABLE = True
        logger.info("PostgreSQL checkpointer å¯ç”¨ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰")
    except ImportError:
        POSTGRES_AVAILABLE = False
        logger.warning("PostgresSaver ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨å†…å­˜å­˜å‚¨ï¼ˆé‡å¯åçŠ¶æ€ä¼šä¸¢å¤±ï¼‰")

# å§‹ç»ˆå¯¼å…¥ MemorySaver ä½œä¸ºå›é€€
from langgraph.checkpoint.memory import MemorySaver

from utils.file import load_prompt_from_yaml, load_prompt_from_md
from utils.draw import draw_workflow
from utils.utils import get_llm_instance
from agent.mcp.mcp_client import client

# ================================
# å…¨å±€å˜é‡
# ================================
llm = get_llm_instance()
llm_with_tools = None
app_graph = None
checkpointer = None
db_pool = None  # PostgreSQL è¿æ¥æ± 

# åŠ è½½æç¤ºè¯æ¨¡æ¿ï¼ˆä½¿ç”¨ md ç‰ˆæœ¬ï¼‰
planner_prompt_data = load_prompt_from_md('planner2.md')
checker_prompt_data = load_prompt_from_md('checker.md')
summarizer_prompt_data = load_prompt_from_md('summarizer.md')

# ================================
# State å®šä¹‰
# ================================
class State(MessagesState):
    run_id: str
    plan: List[Dict[str, Any]]
    current_step: int
    current_tool: Optional[Dict[str, Any]]
    files: List[Dict[str, Any]]
    result: Dict[str, Any]
    replan: bool
    # reply å­—æ®µå·²ç§»é™¤ï¼Œæœ€ç»ˆå›å¤å¯ä»¥ä» messages çš„æœ€åä¸€ä¸ª AIMessage ä¸­è·å–

# ================================
# å·¥å…·å‡½æ•°
# ================================
def extract_text(content):
    """ä»æ¶ˆæ¯å†…å®¹ä¸­æå–æ–‡æœ¬ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰"""
    if isinstance(content, str):
        return content
    elif isinstance(content, list):
        # å¤„ç†åˆ—è¡¨æ ¼å¼ï¼Œå¦‚ [{"type": "text", "text": "..."}]
        text_parts = []
        for item in content:
            if isinstance(item, dict):
                if "text" in item:
                    text_parts.append(item["text"])
                elif "content" in item:
                    text_parts.append(item["content"])
            elif isinstance(item, str):
                text_parts.append(item)
        return "\n".join(text_parts)
    elif isinstance(content, dict):
        return content.get("text", content.get("content", str(content)))
    else:
        return str(content)

async def safe_get_tools() -> list:
    """å®‰å…¨è·å– MCP å·¥å…·åˆ—è¡¨"""
    try:
        return await client.get_tools()
    except Exception as e:
        logger.warning(f"MCP è¿æ¥å¤±è´¥ï¼Œä½¿ç”¨ç©ºå·¥å…·åˆ—è¡¨: {e}")
        return []

async def initialize_llm_with_tools():
    """åˆå§‹åŒ–ç»‘å®šå·¥å…·çš„ LLM"""
    global llm_with_tools
    if llm_with_tools is None:
        mcp_tools = await safe_get_tools()
        llm_with_tools = llm.bind_tools(mcp_tools)
        logger.info(f"LLM å·²ç»‘å®š {len(mcp_tools)} ä¸ªå·¥å…·")
    return llm_with_tools

def extract_text(content):
    """æå–æ–‡æœ¬å†…å®¹"""
    if isinstance(content, str):
        return content
    if isinstance(content, list):
        texts = []
        for item in content:
            if isinstance(item, dict) and item.get("type") == "text":
                texts.append(item.get("text", ""))
        return "\n".join(texts)
    return str(content)

def extract_tool_results(messages):
    """æå–å·¥å…·æ‰§è¡Œç»“æœ"""
    results = []
    for m in messages:
        if isinstance(m, ToolMessage):
            results.append({
                "tool_call_id": m.tool_call_id,
                "content": m.content,
            })
    return results

def get_recent_run_tool_messages(messages, run_id):
    """è·å–å½“å‰ run çš„å·¥å…·æ¶ˆæ¯"""
    results = []
    i = len(messages) - 1
    
    while i >= 0:
        m = messages[i]
        if isinstance(m, ToolMessage):
            if i >= 1:
                prev_msg = messages[i-1]
                if isinstance(prev_msg, AIMessage) and prev_msg.additional_kwargs.get("run_id") == run_id:
                    results.append(m)
                else:
                    break
            else:
                break
            i -= 1
        elif isinstance(m, AIMessage) and m.additional_kwargs.get("run_id") == run_id:
            i -= 1
        else:
            break
    
    return list(reversed(results))

# ================================
# LangGraph èŠ‚ç‚¹å®šä¹‰
# ================================
async def planner(state: State, writer: StreamWriter):
    """ç”Ÿæˆæ‰§è¡Œè®¡åˆ’"""
    messages = state.get("messages", [])
    run_id = state.get("run_id", str(uuid.uuid4()))
    
    # è·å–ç”¨æˆ·è¾“å…¥ï¼ˆå½“å‰ç”¨æˆ·è¾“å…¥ï¼‰
    user_input = ""
    files = state.get("files", [])
    current_user_msg_index = -1
    
    # ä»åå¾€å‰æ‰¾æœ€åä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯
    for i in range(len(messages) - 1, -1, -1):
        msg = messages[i]
        if isinstance(msg, HumanMessage):
            user_input = extract_text(msg.content)
            current_user_msg_index = i
            break
    
    # æ„å»ºå¯¹è¯å†å²ï¼šæ’é™¤å½“å‰ç”¨æˆ·è¾“å…¥åŠå…¶ä¹‹åçš„æ‰€æœ‰æ¶ˆæ¯
    conversation_messages = []
    if current_user_msg_index > 0:
        conversation_messages = messages[:current_user_msg_index]
    else:
        conversation_messages = []
    
    conversation = "\n".join(
        f"{m.type}: {extract_text(m.content)}" for m in conversation_messages
        if m.type in ['human', 'ai'] and hasattr(m, 'content') and extract_text(m.content).strip()
    )
    
    logger.info(f"Planner: ç”¨æˆ·è¾“å…¥: {user_input}")
    
    # å‘é€è®¡åˆ’å¼€å§‹äº‹ä»¶
    writer({"event": "plan_start", "text": "ğŸ¯ æ­£åœ¨ç”Ÿæˆè®¡åˆ’...\n"})
    
    # ç¡®ä¿ LLM å·²ç»‘å®šå·¥å…·
    global llm_with_tools
    if llm_with_tools is None:
        await initialize_llm_with_tools()
    
    # æ ¼å¼åŒ– files ä¸ºå­—ç¬¦ä¸²
    files_str = ""
    if files:
        files_str = "\n".join(
            f"- {f.get('name', '')} ({f.get('path', '')}, {f.get('type', '')})" 
            for f in files if isinstance(f, dict)
        )
    
    # md æ–‡ä»¶è¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥æ ¼å¼åŒ–
    system_prompt = planner_prompt_data.format(
        user_input=user_input,
        conversation=conversation,
        files=files_str if files_str else "æ— "
    )
    
    try:
        response = await llm_with_tools.ainvoke([SystemMessage(content=system_prompt)])
        response_content = response.content if hasattr(response, 'content') else str(response)
        
        # è®°å½•åŸå§‹å“åº”ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        logger.debug(f"Planner: LLM åŸå§‹å“åº”: {response_content[:500]}...")  # åªè®°å½•å‰500å­—ç¬¦
        
        # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
        if not response_content or not response_content.strip():
            logger.error("Planner: LLM è¿”å›ç©ºå“åº”")
            return {
                "plan": [],
                "current_step": 0,
                "messages": messages,
                "run_id": run_id
            }
        
        # å°è¯•è§£æ JSON
        parsed_response = None
        try:
            # é¦–å…ˆå°è¯•ç›´æ¥è§£æ
            parsed_response = json.loads(response_content)
        except json.JSONDecodeError:
            # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æå– JSON å—
            logger.warning("Planner: ç›´æ¥ JSON è§£æå¤±è´¥ï¼Œå°è¯•æå– JSON å—")
            
            # å°è¯•æå– ```json ... ``` ä»£ç å—
            json_block_pattern = r'```(?:json)?\s*(\{.*?\})\s*```'
            json_match = re.search(json_block_pattern, response_content, re.DOTALL)
            if json_match:
                try:
                    parsed_response = json.loads(json_match.group(1))
                    logger.info("Planner: ä»ä»£ç å—ä¸­æˆåŠŸæå– JSON")
                except json.JSONDecodeError:
                    pass
            
            # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•æå–ç¬¬ä¸€ä¸ª { ... } å—
            if parsed_response is None:
                json_obj_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
                json_match = re.search(json_obj_pattern, response_content, re.DOTALL)
                if json_match:
                    try:
                        parsed_response = json.loads(json_match.group(0))
                        logger.info("Planner: ä»æ–‡æœ¬ä¸­æˆåŠŸæå– JSON å¯¹è±¡")
                    except json.JSONDecodeError:
                        pass
            
            # å¦‚æœä»ç„¶å¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶è¿”å›ç©ºè®¡åˆ’
            if parsed_response is None:
                logger.error(f"Planner: æ— æ³•è§£æ JSONï¼Œå“åº”å†…å®¹: {response_content[:200]}")
                raise json.JSONDecodeError("æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆçš„ JSON", response_content, 0)
        
        if not isinstance(parsed_response, dict) or "plan" not in parsed_response:
            logger.warning(f"Planner: LLM è¿”å›æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘ plan å­—æ®µã€‚è§£æç»“æœ: {parsed_response}")
            return {
                "plan": [],
                "current_step": 0,
                "messages": messages,  # ä¸æ·»åŠ é”™è¯¯æ¶ˆæ¯åˆ°å¯¹è¯å†å²
                "run_id": run_id
            }
        
        # å¤„ç†æ‰§è¡Œè®¡åˆ’
        processed_plan = []
        for step in parsed_response["plan"]:
            tool_name = step.get("tool", "") if isinstance(step, dict) else str(step).strip()
            if tool_name:
                processed_plan.append({"tool": tool_name})
        
        logger.info(f"Planner: æ‰§è¡Œè®¡åˆ’: {processed_plan}")
        
        # å‘é€è®¡åˆ’å®Œæˆäº‹ä»¶
        writer({"event": "plan_final", "plan": processed_plan})
        writer({"event": "plan_end", "text": "\n"})
        
        # ç»˜åˆ¶å·¥ä½œæµç¨‹å›¾
        try:
            draw_workflow(parsed_response["plan"])
        except Exception:
            pass
        
        return {
            "plan": processed_plan,
            "current_step": 0,
            "messages": messages,
            "files": files,
            "run_id": run_id
        }
    except json.JSONDecodeError as e:
        logger.error(f"Planner: JSON è§£æå¤±è´¥: {e}", exc_info=True)
        # ä¸æ·»åŠ é”™è¯¯æ¶ˆæ¯åˆ°å¯¹è¯å†å²ï¼Œåªè®°å½•æ—¥å¿—
        return {
            "plan": [],
            "current_step": 0,
            "messages": messages,  # ä¿æŒ messages ä¸å˜ï¼Œä¸æ±¡æŸ“å¯¹è¯å†å²
            "run_id": run_id
        }
    except Exception as e:
        logger.error(f"Planner: è®¡åˆ’ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        # ä¸æ·»åŠ é”™è¯¯æ¶ˆæ¯åˆ°å¯¹è¯å†å²ï¼Œåªè®°å½•æ—¥å¿—
        return {
            "plan": [],
            "current_step": 0,
            "messages": messages,  # ä¿æŒ messages ä¸å˜ï¼Œä¸æ±¡æŸ“å¯¹è¯å†å²
            "run_id": run_id
        }

async def executor(state: State):
    """æ‰§è¡Œè®¡åˆ’æ­¥éª¤"""
    global llm_with_tools
    
    plan = state.get("plan", [])
    current_step = state.get("current_step", 0)
    messages = state.get("messages", [])
    run_id = state.get("run_id", str(uuid.uuid4()))
    
    # ç©ºè®¡åˆ’å¤„ç†
    if not plan:
        return {
            "next": "summarizer",
            "plan": plan,
            "messages": messages,
        }
    
    # è§£æä¸Šä¸€ä¸ªå·¥å…·æ‰§è¡Œç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
    is_retry = False
    retry_count = state.get("retry_count", 0)
    
    if messages and isinstance(messages[-1], ToolMessage):
        try:
            # å¤„ç†ä¸åŒçš„ content æ ¼å¼
            content = messages[-1].content
            if isinstance(content, list) and len(content) > 0:
                raw_text = content[0].get("text", "") if isinstance(content[0], dict) else str(content[0])
            else:
                raw_text = str(content)
            
            result_json = json.loads(raw_text)
            success = result_json.get("status") == "success"
            # tool_failed = not success
            
            # å¦‚æœå·¥å…·æ‰§è¡Œå¤±è´¥ä¸”å¯ä»¥é‡è¯•
            if not success and retry_count < 1:
                logger.info(f"Executor: å·¥å…·æ‰§è¡Œå¤±è´¥ï¼Œå‡†å¤‡é‡è¯• ({retry_count + 1}/1)")
                is_retry = True
                
                # ç§»é™¤å¤±è´¥çš„ ToolMessage å’Œå¯¹åº”çš„ AIMessageï¼ˆåŒ…å« tool_callï¼‰
                cleaned_messages = messages[:-1]  # ç§»é™¤ ToolMessage
                if cleaned_messages and isinstance(cleaned_messages[-1], AIMessage) and cleaned_messages[-1].tool_calls:
                    cleaned_messages = cleaned_messages[:-1]  # ç§»é™¤åŒ…å« tool_call çš„ AIMessage
                    logger.info(f"Executor: å·²ç§»é™¤å¤±è´¥çš„ ToolMessage å’Œ AIMessageï¼Œå‡†å¤‡é‡æ–°ç”Ÿæˆ tool_call")
                
                # æ›´æ–° messagesï¼Œä¿æŒ current_step ä¸å˜ï¼Œç»§ç»­æ‰§è¡Œåé¢çš„ä»£ç é‡æ–°ç”Ÿæˆ tool_call
                messages = cleaned_messages
                
            # å¦‚æœé‡è¯•æ¬¡æ•°å·²ç”¨å®Œä»ç„¶å¤±è´¥ï¼Œè·³è¿‡å½“å‰æ­¥éª¤
            elif not success and retry_count >= 1:
                logger.warning(f"Executor: å·¥å…·æ‰§è¡Œå¤±è´¥ä¸”é‡è¯•æ¬¡æ•°å·²ç”¨å®Œï¼Œè·³è¿‡å½“å‰æ­¥éª¤")
                return {
                    "messages": messages,
                    "current_step": current_step + 1,  # è·³è¿‡å½“å‰æ­¥éª¤
                    "plan": plan,
                    "retry_count": 0,  # é‡ç½®é‡è¯•è®¡æ•°
                    "next": "check" if current_step + 1 >= len(plan) else "tools",
                }
                
        except (json.JSONDecodeError, KeyError, IndexError, TypeError) as e:
            logger.error(f"Executor: è§£æå·¥å…·ç»“æœå¤±è´¥: {e}, content: {messages[-1].content}")
            # è§£æå¤±è´¥ä¹Ÿè§†ä¸ºå·¥å…·æ‰§è¡Œå¤±è´¥
            if retry_count < 1:
                logger.info(f"Executor: è§£æå¤±è´¥ï¼Œå‡†å¤‡é‡è¯• ({retry_count + 1}/1)")
                is_retry = True
                cleaned_messages = messages[:-1]
                if cleaned_messages and isinstance(cleaned_messages[-1], AIMessage) and cleaned_messages[-1].tool_calls:
                    cleaned_messages = cleaned_messages[:-1]
                messages = cleaned_messages
            else:
                logger.warning(f"Executor: è§£æå¤±è´¥ä¸”é‡è¯•æ¬¡æ•°å·²ç”¨å®Œï¼Œè·³è¿‡å½“å‰æ­¥éª¤")
                return {
                    "messages": messages,
                    "current_step": current_step + 1,
                    "plan": plan,
                    "retry_count": 0,
                    "next": "check" if current_step + 1 >= len(plan) else "tools",
                }
    
    # è®¡åˆ’æ‰§è¡Œå®Œæˆ
    if current_step >= len(plan):
        return {
            "current_step": current_step,
            "next": "check",
            "messages": messages,
        }
    
    # å½“å‰æ­¥éª¤
    step = plan[current_step]
    tool_name = step["tool"]
    logger.info(f"Executor: Step {current_step + 1}/{len(plan)} â†’ {tool_name}")
    
    # æ„å»ºæ–‡ä»¶ä¸Šä¸‹æ–‡
    files = state.get("files", [])
    logger.info(f"Executor: å½“å‰æ–‡ä»¶åˆ—è¡¨: {files}")
    file_context = ""
    if files:
        # æ›´æ˜ç¡®åœ°æä¾›æ–‡ä»¶ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯è·¯å¾„
        file_list = []
        for f in files:
            file_name = f.get('name', 'æœªçŸ¥æ–‡ä»¶')
            file_path = f.get('path', '')
            file_type = f.get('type', '')
            
            # è®°å½•åŸå§‹è·¯å¾„
            logger.info(f"Executor: å¤„ç†æ–‡ä»¶ - åç§°: {file_name}, åŸå§‹è·¯å¾„: {file_path}, ç±»å‹: {file_type}")
            
            # ç¡®ä¿è·¯å¾„æ˜¯ç»å¯¹è·¯å¾„
            if file_path:
                if not os.path.isabs(file_path):
                    # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
                    file_path = os.path.abspath(file_path)
                    logger.info(f"Executor: è½¬æ¢ä¸ºç»å¯¹è·¯å¾„: {file_path}")
                
                # éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if os.path.exists(file_path):
                    logger.info(f"Executor: æ–‡ä»¶å­˜åœ¨: {file_path}")
                else:
                    logger.warning(f"Executor: æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            else:
                logger.warning(f"Executor: æ–‡ä»¶è·¯å¾„ä¸ºç©º: {file_name}")
            
            file_list.append(f"æ–‡ä»¶å: {file_name}, è·¯å¾„: {file_path}, ç±»å‹: {file_type}")
        
        file_context = "å½“å‰å¯ç”¨æ–‡ä»¶ä¿¡æ¯ï¼š\n" + "\n".join(file_list)
        file_context += "\n\né‡è¦ï¼šå¦‚æœå·¥å…·éœ€è¦æ–‡ä»¶è·¯å¾„å‚æ•°ï¼ˆå¦‚ image_pathï¼‰ï¼Œè¯·ä½¿ç”¨ä¸Šé¢æä¾›çš„å®Œæ•´è·¯å¾„ã€‚"
        logger.info(f"Executor: æ–‡ä»¶ä¸Šä¸‹æ–‡: {file_context}")
    else:
        logger.info("Executor: æ²¡æœ‰å¯ç”¨æ–‡ä»¶")
    
    # è®© LLM ç”Ÿæˆ tool_call
    llm_messages = messages + [
        SystemMessage(content=file_context) if file_context else SystemMessage(content=""),
        HumanMessage(content=f"è¯·è°ƒç”¨å·¥å…· `{tool_name}`ï¼Œè‡ªåŠ¨ä»ä¸Šä¸‹æ–‡ä¸­æå–æ‰€éœ€å‚æ•°ã€‚å¦‚æœå·¥å…·éœ€è¦æ–‡ä»¶è·¯å¾„ï¼Œè¯·ä½¿ç”¨ä¸Šé¢æä¾›çš„å®Œæ•´æ–‡ä»¶è·¯å¾„ã€‚ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæ–‡æœ¬ã€‚")
    ]
    
    if llm_with_tools is None:
        await initialize_llm_with_tools()
    
    response = await llm_with_tools.ainvoke(
        llm_messages,
        tool_choice={"type": "function", "function": {"name": tool_name}},
    )
    
    if not response.tool_calls:
        raise RuntimeError(f"Executor: LLM æœªç”Ÿæˆ tool_call: {tool_name}")
    
    response.additional_kwargs["run_id"] = run_id
    
    # å¦‚æœæ˜¯é‡è¯•ï¼Œæ›´æ–° retry_count å¹¶ä¿æŒ current_step ä¸å˜ï¼›å¦åˆ™é‡ç½®ä¸º 0 å¹¶å‰è¿›æ­¥éª¤
    if is_retry:
        new_retry_count = retry_count + 1
        new_current_step = current_step  # é‡è¯•æ—¶ä¿æŒæ­¥éª¤ä¸å˜ï¼Œé‡æ–°æ‰§è¡Œå½“å‰æ­¥éª¤
    else:
        new_retry_count = 0
        new_current_step = current_step + 1  # æ­£å¸¸æ‰§è¡Œæ—¶å‰è¿›æ­¥éª¤
    
    return {
        "messages": messages + [response],
        "current_step": new_current_step,
        "plan": plan,
        "run_id": run_id,
        "next": "tools",
        "retry_count": new_retry_count
    }

async def checker(state: State):
    """æ£€æŸ¥æ‰§è¡Œç»“æœ"""
    messages = state["messages"]
    run_id = state.get("run_id", "")
    
    # è·å–ç”¨æˆ·æŸ¥è¯¢ï¼ˆå½“å‰ç”¨æˆ·è¾“å…¥ï¼‰
    user_query = None
    current_user_msg_index = -1
    for i in range(len(messages) - 1, -1, -1):
        msg = messages[i]
        if isinstance(msg, HumanMessage):
            user_query = extract_text(msg.content) if hasattr(msg, 'content') else ""
            current_user_msg_index = i
            break
    
    # æå–å·¥å…·æ‰§è¡Œç»“æœ
    recent_tool_messages = get_recent_run_tool_messages(messages, run_id)
    tool_results = extract_tool_results(recent_tool_messages)
    final_result = json.dumps(tool_results, ensure_ascii=False, indent=2) if tool_results else ""
    
    # æ„å»ºå¯¹è¯å†å²ï¼šæ’é™¤å½“å‰ç”¨æˆ·è¾“å…¥åŠå…¶ä¹‹åçš„æ‰€æœ‰æ¶ˆæ¯
    conversation_messages = []
    if current_user_msg_index > 0:
        conversation_messages = messages[:current_user_msg_index]
    else:
        conversation_messages = []
    
    conversation = "\n".join(
        f"{m.type}: {extract_text(m.content)}" for m in conversation_messages
        if hasattr(m, 'type') and m.type in ['human', 'ai'] and hasattr(m, 'content') and extract_text(m.content).strip()
    )
    
    logger.info(f"Checker: ç”¨æˆ·é—®é¢˜: {user_query}")
    logger.info(f"Checker: æ‰§è¡Œç»“æœ: {final_result[:200]}...")
    
    # md æ–‡ä»¶è¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥æ ¼å¼åŒ–
    system_prompt = checker_prompt_data.format(
        user_query=user_query,
        final_result=final_result,
        conversation=conversation
    )
    evaluation_prompt = [
        SystemMessage(content=system_prompt)
    ]
    
    try:
        response = await llm.ainvoke(evaluation_prompt)
        response_content = response.content.strip() if hasattr(response, 'content') else str(response).strip()
        
        # æ£€æŸ¥å“åº”å†…å®¹æ˜¯å¦ä¸ºç©º
        if not response_content:
            logger.warning("Checker: LLM è¿”å›ç©ºå†…å®¹ï¼Œä½¿ç”¨é»˜è®¤è¯„ä¼°ç»“æœ")
            evaluation_result = {
                "satisfies_needs": True,
                "reason": "LLM è¿”å›ç©ºå†…å®¹ï¼Œé»˜è®¤è®¤ä¸ºæ»¡è¶³éœ€æ±‚",
                "needs_replan": False
            }
        else:
            # å°è¯•è§£æ JSON
            try:
                evaluation_result = json.loads(response_content)
            except json.JSONDecodeError as json_err:
                logger.error(f"Checker: JSON è§£æå¤±è´¥ï¼Œå“åº”å†…å®¹: {response_content[:200]}")
                logger.error(f"Checker: JSON è§£æé”™è¯¯è¯¦æƒ…: {json_err}")
                
                # å°è¯•æå– JSON éƒ¨åˆ†ï¼ˆå¯èƒ½åŒ…å«å…¶ä»–æ–‡æœ¬ï¼‰
                import re
                json_match = re.search(r'\{.*\}', response_content, re.DOTALL)
                if json_match:
                    try:
                        evaluation_result = json.loads(json_match.group())
                        logger.info("Checker: ä»å“åº”ä¸­æå–å¹¶è§£æ JSON æˆåŠŸ")
                    except json.JSONDecodeError:
                        logger.error("Checker: æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆ JSONï¼Œä½¿ç”¨é»˜è®¤ç»“æœ")
                        evaluation_result = {
                            "satisfies_needs": True,
                            "reason": "æ— æ³•è§£æ LLM å“åº”ï¼Œé»˜è®¤è®¤ä¸ºæ»¡è¶³éœ€æ±‚",
                            "needs_replan": False
                        }
                else:
                    logger.error("Checker: å“åº”ä¸­æœªæ‰¾åˆ° JSON æ ¼å¼ï¼Œä½¿ç”¨é»˜è®¤ç»“æœ")
                    evaluation_result = {
                        "satisfies_needs": True,
                        "reason": "å“åº”æ ¼å¼é”™è¯¯ï¼Œé»˜è®¤è®¤ä¸ºæ»¡è¶³éœ€æ±‚",
                        "needs_replan": False
                    }
        
        # ç¡®ä¿å¿…è¦çš„å­—æ®µå­˜åœ¨
        if "satisfies_needs" not in evaluation_result:
            evaluation_result["satisfies_needs"] = True
        if "needs_replan" not in evaluation_result:
            evaluation_result["needs_replan"] = False
        
        logger.info(f"Checker: æ»¡è¶³éœ€æ±‚: {evaluation_result['satisfies_needs']}, éœ€è¦é‡è§„åˆ’: {evaluation_result.get('needs_replan', False)}")
        
        # æš‚ä¸é‡è§„åˆ’
        evaluation_result["needs_replan"] = False
        
        return {
            "replan": evaluation_result["needs_replan"],
            "next": "replan" if evaluation_result["needs_replan"] else "end",
            "plan": state.get("plan", []),
            "messages": messages
        }
    except Exception as e:
        logger.error(f"Checker é”™è¯¯: {e}", exc_info=True)
        return {
            "replan": False,
            "next": "end",
            "plan": state.get("plan", []),
            "messages": messages
        }

async def summarizer(state: State, writer: StreamWriter):
    """ç”Ÿæˆæœ€ç»ˆå›å¤ - æµå¼è¾“å‡º"""
    messages = state["messages"]
    run_id = state.get("run_id", "")
    
    # è·å–ç”¨æˆ·æŸ¥è¯¢ï¼ˆå½“å‰ç”¨æˆ·è¾“å…¥ï¼‰
    user_query = ""
    current_user_msg_index = -1
    for i in range(len(messages) - 1, -1, -1):
        msg = messages[i]
        if hasattr(msg, 'type') and msg.type == "human":
            user_query = extract_text(msg.content) if hasattr(msg, 'content') else ""
            current_user_msg_index = i
            break
    
    # æå–å·¥å…·æ‰§è¡Œç»“æœ
    recent_tool_messages = get_recent_run_tool_messages(messages, run_id)
    tool_results = extract_tool_results(recent_tool_messages)
    final_result = json.dumps(tool_results, ensure_ascii=False, indent=2) if tool_results else ""
    
    # æ„å»ºå¯¹è¯å†å²ï¼šæ’é™¤å½“å‰ç”¨æˆ·è¾“å…¥åŠå…¶ä¹‹åçš„æ‰€æœ‰æ¶ˆæ¯ï¼ˆåŒ…æ‹¬å·¥å…·è°ƒç”¨ã€AIå›å¤ç­‰ï¼‰
    # åªä¿ç•™å½“å‰ç”¨æˆ·è¾“å…¥ä¹‹å‰çš„å¯¹è¯å†å²
    conversation_messages = []
    if current_user_msg_index > 0:
        # åªå–å½“å‰ç”¨æˆ·è¾“å…¥ä¹‹å‰çš„æ¶ˆæ¯
        conversation_messages = messages[:current_user_msg_index]
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å½“å‰ç”¨æˆ·è¾“å…¥ï¼Œæˆ–è€…å½“å‰ç”¨æˆ·è¾“å…¥æ˜¯ç¬¬ä¸€æ¡æ¶ˆæ¯ï¼Œåˆ™æ²¡æœ‰å†å²å¯¹è¯
        conversation_messages = []
    
    # æ ¼å¼åŒ–å¯¹è¯å†å²ï¼ŒåªåŒ…å« human å’Œ ai ç±»å‹çš„æ¶ˆæ¯
    conversation = "\n".join(
        f"{m.type}: {extract_text(m.content)}" for m in conversation_messages
        if hasattr(m, 'type') and m.type in ['human', 'ai'] and hasattr(m, 'content') and extract_text(m.content).strip()
    )
    
    logger.info(f"Summarizer: å½“å‰ç”¨æˆ·è¾“å…¥: {user_query}")
    logger.info(f"Summarizer: å¯¹è¯å†å²é•¿åº¦: {len(conversation_messages)} æ¡æ¶ˆæ¯")
    logger.info(f"Summarizer: å¯¹è¯å†å²: {conversation[:200]}..." if conversation else "Summarizer: æ— å¯¹è¯å†å²")
    
    logger.info("Summarizer: ç”Ÿæˆæœ€ç»ˆå›å¤")
    
    # md æ–‡ä»¶è¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥æ ¼å¼åŒ–
    system_prompt = summarizer_prompt_data.format(
        user_query=user_query,
        final_result=final_result,
        conversation=conversation
    )
    summary_prompt = [
        SystemMessage(content=system_prompt)
    ]
    
    accumulated_reply = ""
    final_messages = messages.copy()
    
    try:
        if hasattr(llm, "astream"):
            async for chunk in llm.astream(summary_prompt):
                if hasattr(chunk, 'content') and chunk.content:
                    accumulated_reply += chunk.content
                    # ä½¿ç”¨ StreamWriter å‘é€æµå¼æ•°æ®ï¼ˆåŒæ­¥è°ƒç”¨ï¼‰
                    writer({
                        "event_type": "custom_stream",
                        "reply": chunk.content,
                        "is_partial": True
                    })
        else:
            response = await llm.ainvoke(summary_prompt)
            accumulated_reply = response.content if hasattr(response, 'content') else str(response)
            writer({
                "event_type": "custom_stream",
                "reply": accumulated_reply,
                "is_partial": False
            })
    except Exception as e:
        logger.error(f"Summarizer é”™è¯¯: {e}")
        accumulated_reply = final_result or "æŠ±æ­‰ï¼Œç”Ÿæˆå›å¤æ—¶å‡ºé”™ã€‚"
        writer({
            "event_type": "custom_stream",
            "reply": accumulated_reply,
            "is_partial": False
        })
    
    final_messages.append(AIMessage(content=accumulated_reply))
    
    # reply å­—æ®µå·²ç§»é™¤ï¼Œæœ€ç»ˆå›å¤å·²åŒ…å«åœ¨ messages çš„æœ€åä¸€ä¸ª AIMessage ä¸­
    return {
        "messages": final_messages,
        "plan": state.get("plan", [])
    }

# ================================
# å›¾åˆå§‹åŒ–
# ================================
async def init_checkpointer():
    """åˆå§‹åŒ– checkpointerï¼ˆæ”¯æŒ PostgreSQL æˆ–å†…å­˜ï¼‰"""
    global checkpointer, db_pool
    
    if checkpointer is not None:
        return checkpointer
    
    # ä»ç¯å¢ƒå˜é‡è¯»å–æ•°æ®åº“é…ç½®
    db_conn_string = os.getenv("DB_CONN_STRING", "")
    use_postgres = os.getenv("USE_POSTGRES", "false").lower() == "true"
    
    if use_postgres and POSTGRES_AVAILABLE and db_conn_string:
        try:
            logger.info("ä½¿ç”¨ PostgreSQL ä½œä¸º checkpointer")
            
            # å¦‚æœæ”¯æŒè¿æ¥æ± ï¼Œä½¿ç”¨è¿æ¥æ± æ–¹å¼ï¼ˆæ¨èï¼‰
            if AsyncPostgresSaver is not None and AsyncConnectionPool is not None:
                # ä½¿ç”¨è¿æ¥æ± æ–¹å¼
                logger.info("ä½¿ç”¨è¿æ¥æ± æ–¹å¼åˆå§‹åŒ– PostgreSQL checkpointer")
                
                # ä»ç¯å¢ƒå˜é‡è¯»å–è¿æ¥æ± é…ç½®
                pool_max_size = int(os.getenv("DB_POOL_MAX_SIZE", "20"))
                pool_min_size = int(os.getenv("DB_POOL_MIN_SIZE", "5"))
                pool_timeout = int(os.getenv("DB_POOL_TIMEOUT", "30"))
                
                # å¤„ç†è¿æ¥å­—ç¬¦ä¸²æ ¼å¼ï¼ˆpsycopg éœ€è¦ postgresql:// æ ¼å¼ï¼Œä¸æ”¯æŒ postgresql+asyncpg://ï¼‰
                # å¦‚æœè¿æ¥å­—ç¬¦ä¸²åŒ…å« postgresql+asyncpg://ï¼Œéœ€è¦è½¬æ¢ä¸º postgresql://
                conninfo = db_conn_string.replace("postgresql+asyncpg://", "postgresql://")
                
                # å…ˆä½¿ç”¨ä¸´æ—¶è¿æ¥æ‰§è¡Œ setupï¼ˆå› ä¸º CREATE INDEX CONCURRENTLY ä¸èƒ½åœ¨äº‹åŠ¡ä¸­æ‰§è¡Œï¼‰
                # ä½¿ç”¨å•ç‹¬çš„è¿æ¥åœ¨ autocommit æ¨¡å¼ä¸‹æ‰§è¡Œ setup
                logger.info("ä½¿ç”¨ä¸´æ—¶è¿æ¥åˆå§‹åŒ–æ•°æ®åº“è¡¨...")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶é‡æ–°åˆ›å»ºè¡¨
                force_recreate = os.getenv("DB_FORCE_RECREATE", "false").lower() == "true"
                if force_recreate:
                    logger.warning("âš ï¸  DB_FORCE_RECREATE=trueï¼Œå°†åˆ é™¤å¹¶é‡æ–°åˆ›å»ºæ‰€æœ‰è¡¨ï¼ˆä¼šä¸¢å¤±æ•°æ®ï¼‰")
                    try:
                        from psycopg import AsyncConnection
                        async with await AsyncConnection.connect(conninfo, autocommit=True) as temp_conn:
                            async with temp_conn.cursor() as cur:
                                # åˆ é™¤æ‰€æœ‰ç›¸å…³è¡¨
                                await cur.execute("DROP TABLE IF EXISTS checkpoint_writes CASCADE")
                                await cur.execute("DROP TABLE IF EXISTS checkpoint_blobs CASCADE")
                                await cur.execute("DROP TABLE IF EXISTS checkpoints CASCADE")
                                await cur.execute("DROP TABLE IF EXISTS checkpoint_migrations CASCADE")
                                logger.info("å·²åˆ é™¤æ—§è¡¨")
                    except Exception as drop_error:
                        logger.warning(f"åˆ é™¤æ—§è¡¨æ—¶å‡ºé”™ï¼ˆå¯èƒ½è¡¨ä¸å­˜åœ¨ï¼‰: {drop_error}")
                
                setup_success = False
                try:
                    from psycopg import AsyncConnection
                    # åˆ›å»ºä¸´æ—¶è¿æ¥ï¼Œä½¿ç”¨ autocommit æ¨¡å¼
                    # åœ¨ psycopg 3.x ä¸­ï¼Œautocommit é€šè¿‡è¿æ¥å‚æ•°è®¾ç½®
                    async with await AsyncConnection.connect(conninfo, autocommit=True) as temp_conn:
                        # åˆ›å»ºä¸´æ—¶ checkpointer æ¥æ‰§è¡Œ setup
                        temp_checkpointer = AsyncPostgresSaver(temp_conn)
                        await temp_checkpointer.setup()
                        setup_success = True
                        logger.info("æ•°æ®åº“è¡¨åˆå§‹åŒ–å®Œæˆï¼ˆä½¿ç”¨ autocommit æ¨¡å¼ï¼‰")
                except Exception as setup_error:
                    # å¦‚æœ setup å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ from_conn_string æ–¹å¼ï¼ˆå¯èƒ½è¡¨å·²ç»å­˜åœ¨ï¼‰
                    logger.warning(f"ä½¿ç”¨ autocommit æ¨¡å¼ setup å¤±è´¥: {setup_error}ï¼Œå°è¯•å…¶ä»–æ–¹å¼...")
                    try:
                        # å°è¯•ä½¿ç”¨ from_conn_string æ–¹å¼ï¼ˆå¦‚æœè¡¨å·²å­˜åœ¨ï¼Œä¸ä¼šæŠ¥é”™ï¼‰
                        temp_checkpointer = await AsyncPostgresSaver.from_conn_string(conninfo).asetup()
                        setup_success = True
                        logger.info("æ•°æ®åº“è¡¨åˆå§‹åŒ–å®Œæˆï¼ˆä½¿ç”¨ from_conn_string æ–¹å¼ï¼‰")
                    except Exception as e2:
                        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œè®°å½•é”™è¯¯
                        logger.error(f"æ•°æ®åº“è¡¨åˆå§‹åŒ–å¤±è´¥: {e2}")
                        # éªŒè¯è¡¨æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨ä½†ç»“æ„ä¸å®Œæ•´ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨ä¿®å¤
                        try:
                            from psycopg import AsyncConnection
                            async with await AsyncConnection.connect(conninfo) as verify_conn:
                                async with verify_conn.cursor() as cur:
                                    await cur.execute("""
                                        SELECT table_name 
                                        FROM information_schema.tables 
                                        WHERE table_schema = 'public' 
                                        AND table_name = 'checkpoints'
                                    """)
                                    table_exists = await cur.fetchone()
                                    if table_exists:
                                        logger.warning("checkpoints è¡¨å·²å­˜åœ¨ï¼Œä½†å¯èƒ½ç»“æ„ä¸å®Œæ•´ã€‚å»ºè®®è¿è¡Œåˆå§‹åŒ–è„šæœ¬ä¿®å¤ã€‚")
                                        logger.warning("å¯ä»¥è¿è¡Œ: python3 scripts/init_postgres_tables.py")
                        except Exception:
                            pass
                        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä»£ç ç»§ç»­ï¼Œä½†ä¼šå›é€€åˆ°å†…å­˜å­˜å‚¨
                        raise Exception(f"æ•°æ®åº“è¡¨åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•ä½¿ç”¨ PostgreSQL checkpointer: {e2}")
                
                if not setup_success:
                    raise Exception("æ•°æ®åº“è¡¨åˆå§‹åŒ–å¤±è´¥")
                
                # åˆ›å»ºè¿æ¥æ± 
                db_pool = AsyncConnectionPool(
                    conninfo=conninfo,
                    max_size=pool_max_size,
                    min_size=pool_min_size,
                    timeout=pool_timeout,
                    # å¯é€‰ï¼šæ·»åŠ è¿æ¥å‚æ•°
                    kwargs={
                        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ é¢å¤–çš„è¿æ¥å‚æ•°
                    }
                )
                
                # æ‰“å¼€è¿æ¥æ± ï¼ˆç¡®ä¿è¿æ¥æ± åœ¨ä½¿ç”¨å‰å·²å‡†å¤‡å¥½ï¼‰
                await db_pool.open()
                
                # ä½¿ç”¨è¿æ¥æ± åˆ›å»º checkpointerï¼ˆè¡¨å·²ç»åˆå§‹åŒ–ï¼Œä¸éœ€è¦å†æ¬¡ setupï¼‰
                checkpointer = AsyncPostgresSaver(db_pool)
                
                # éªŒè¯è¡¨ç»“æ„æ˜¯å¦æ­£ç¡®ï¼ˆæ£€æŸ¥ä¸»é”®çº¦æŸæ˜¯å¦å­˜åœ¨ï¼‰
                try:
                    from psycopg import AsyncConnection
                    async with await AsyncConnection.connect(conninfo) as verify_conn:
                        async with verify_conn.cursor() as cur:
                            # æ£€æŸ¥ checkpoints è¡¨çš„ä¸»é”®çº¦æŸ
                            await cur.execute("""
                                SELECT constraint_name, constraint_type
                                FROM information_schema.table_constraints
                                WHERE table_schema = 'public' 
                                AND table_name = 'checkpoints'
                                AND constraint_type = 'PRIMARY KEY'
                            """)
                            pk_exists = await cur.fetchone()
                            if not pk_exists:
                                logger.warning("âš ï¸  checkpoints è¡¨ç¼ºå°‘ä¸»é”®çº¦æŸï¼Œå¯èƒ½å¯¼è‡´ ON CONFLICT é”™è¯¯")
                                logger.warning("å»ºè®®è¿è¡Œåˆå§‹åŒ–è„šæœ¬ä¿®å¤: python3 scripts/init_postgres_tables.py")
                except Exception as verify_error:
                    logger.warning(f"éªŒè¯è¡¨ç»“æ„æ—¶å‡ºé”™: {verify_error}")
                
                logger.info(f"PostgreSQL checkpointer åˆå§‹åŒ–æˆåŠŸï¼ˆè¿æ¥æ± å¤§å°: {pool_min_size}-{pool_max_size}ï¼‰")
                
            elif AsyncPostgresSaver is not None:
                # å›é€€åˆ°ç›´æ¥è¿æ¥å­—ç¬¦ä¸²æ–¹å¼
                logger.info("ä½¿ç”¨ç›´æ¥è¿æ¥å­—ç¬¦ä¸²æ–¹å¼åˆå§‹åŒ– PostgreSQL checkpointer")
                checkpointer = await AsyncPostgresSaver.from_conn_string(db_conn_string).asetup()
                logger.info("PostgreSQL checkpointer åˆå§‹åŒ–æˆåŠŸ")
                
            elif PostgresSaver is not None:
                # ä½¿ç”¨åŒæ­¥ç‰ˆæœ¬
                checkpointer = PostgresSaver.from_conn_string(db_conn_string)
                # åŒæ­¥ç‰ˆæœ¬å¯èƒ½éœ€è¦æ‰‹åŠ¨ setup
                if hasattr(checkpointer, 'asetup'):
                    await checkpointer.asetup()
                elif hasattr(checkpointer, 'setup'):
                    # å¦‚æœæ˜¯åŒæ­¥æ–¹æ³•ï¼Œéœ€è¦åœ¨åŒæ­¥ä¸Šä¸‹æ–‡ä¸­è°ƒç”¨
                    import asyncio
                    await asyncio.to_thread(checkpointer.setup)
                logger.info("PostgreSQL checkpointer åˆå§‹åŒ–æˆåŠŸï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰")
            else:
                raise ImportError("PostgresSaver ç±»ä¸å¯ç”¨")
                
        except Exception as e:
            logger.error(f"PostgreSQL checkpointer åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå›é€€åˆ°å†…å­˜å­˜å‚¨", exc_info=True)
            # å¦‚æœè¿æ¥æ± å·²åˆ›å»ºï¼Œéœ€è¦æ¸…ç†
            if db_pool is not None:
                try:
                    await db_pool.close()
                except Exception:
                    pass
                db_pool = None
            
            # å¦‚æœæ˜¯è¡¨ç»“æ„é—®é¢˜ï¼Œæä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            error_str = str(e).lower()
            if "constraint" in error_str or "conflict" in error_str or "index" in error_str:
                logger.error("=" * 60)
                logger.error("æ•°æ®åº“è¡¨ç»“æ„å¯èƒ½ä¸å®Œæ•´ï¼")
                logger.error("å»ºè®®æ‰§è¡Œä»¥ä¸‹æ“ä½œä¿®å¤ï¼š")
                logger.error("1. è¿è¡Œåˆå§‹åŒ–è„šæœ¬: python3 scripts/init_postgres_tables.py")
                logger.error("2. æˆ–è€…åˆ é™¤æ—§è¡¨åé‡æ–°å¯åŠ¨æœåŠ¡ï¼ˆä¼šä¸¢å¤±æ•°æ®ï¼‰")
                logger.error("=" * 60)
            
            checkpointer = MemorySaver()
    else:
        if use_postgres:
            logger.warning("PostgreSQL é…ç½®å­˜åœ¨ä½† PostgresSaver ä¸å¯ç”¨ï¼Œä½¿ç”¨å†…å­˜å­˜å‚¨")
        else:
            logger.info("ä½¿ç”¨å†…å­˜å­˜å‚¨ï¼ˆMemorySaverï¼‰ï¼Œé‡å¯åçŠ¶æ€ä¼šä¸¢å¤±")
        checkpointer = MemorySaver()
    
    return checkpointer

async def init_graph():
    """å»¶è¿Ÿåˆå§‹åŒ– LangGraph å›¾"""
    global app_graph
    
    # åˆå§‹åŒ– checkpointer
    await init_checkpointer()
    
    # åˆå§‹åŒ–å·¥å…·
    tools = await safe_get_tools()
    logger.info(f"åˆå§‹åŒ–å·¥å…·åˆ—è¡¨: {len(tools)} ä¸ªå·¥å…·")
    tool_node = ToolNode(tools)
    
    # æ„å»ºå›¾
    graph = StateGraph(State)
    
    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("planner", planner, aflow=True)
    graph.add_node("executor", executor)
    graph.add_node("tools", tool_node)
    graph.add_node("checker", checker, aflow=True)
    graph.add_node("summarizer", summarizer, aflow=True)
    
    # è®¾ç½®å…¥å£ç‚¹
    graph.set_entry_point("planner")
    
    # è¿æ¥èŠ‚ç‚¹
    graph.add_edge("planner", "executor")
    
    graph.add_conditional_edges(
        "executor",
        lambda x: x.get("next", "end"),
        {
            "tools": "tools",
            "check": "checker",
            "summarizer": "summarizer",
            "end": END
        }
    )
    
    graph.add_edge("tools", "executor")
    
    graph.add_conditional_edges(
        "checker",
        lambda x: x.get("next", "end"),
        {
            "replan": "planner",
            "end": "summarizer"
        }
    )
    
    # summarizer å®Œæˆåç»“æŸ
    graph.add_edge("summarizer", END)
    
    # ç¼–è¯‘å›¾ï¼ˆä½¿ç”¨ checkpointer å®ç°æŒä¹…åŒ–è®°å¿†ï¼‰
    app_graph = graph.compile(name="bnerc_agent", checkpointer=checkpointer)
    
    logger.info("LangGraph å›¾ç¼–è¯‘å®Œæˆï¼ˆå·²å¯ç”¨æŒä¹…åŒ–è®°å¿†ï¼‰")
    return app_graph

# ================================
# FastAPI ç”Ÿå‘½å‘¨æœŸç®¡ç†
# ================================
@asynccontextmanager
async def lifespan(app: FastAPI):
    """ASGI ç”Ÿå‘½å‘¨æœŸäº‹ä»¶å¤„ç† - å¯åŠ¨å’Œå…³é—­"""
    # å¯åŠ¨æ—¶æ‰§è¡Œ
    logger.info("åº”ç”¨å¯åŠ¨ä¸­...")
    try:
        # é¢„åˆå§‹åŒ–å›¾ï¼ˆå¯é€‰ï¼Œä¹Ÿå¯ä»¥å»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        # await init_graph()
        logger.info("åº”ç”¨å¯åŠ¨å®Œæˆ")
    except Exception as e:
        logger.error(f"åº”ç”¨å¯åŠ¨å¤±è´¥: {e}", exc_info=True)
        raise
    
    yield
    
    # å…³é—­æ—¶æ‰§è¡Œ
    logger.info("åº”ç”¨å…³é—­ä¸­...")
    try:
        # æ¸…ç†èµ„æº
        global app_graph, llm_with_tools, checkpointer, db_pool
        app_graph = None
        llm_with_tools = None
        
        # å…³é—­è¿æ¥æ± ï¼ˆå¦‚æœä½¿ç”¨è¿æ¥æ± ï¼‰
        if db_pool is not None:
            try:
                logger.info("æ­£åœ¨å…³é—­ PostgreSQL è¿æ¥æ± ...")
                await db_pool.close()
                db_pool = None
                logger.info("PostgreSQL è¿æ¥æ± å·²å…³é—­")
            except Exception as e:
                logger.error(f"å…³é—­è¿æ¥æ± æ—¶å‡ºé”™: {e}", exc_info=True)
        
        checkpointer = None
        logger.info("åº”ç”¨å…³é—­å®Œæˆ")
    except Exception as e:
        logger.error(f"åº”ç”¨å…³é—­æ—¶å‡ºé”™: {e}", exc_info=True)

# ================================
# FastAPI åº”ç”¨
# ================================
# ä»ç¯å¢ƒå˜é‡è¯»å– CORS é…ç½®ï¼Œç”Ÿäº§ç¯å¢ƒåº”é™åˆ¶åŸŸå
cors_origins = os.getenv("CORS_ORIGINS", "*").split(",") if os.getenv("CORS_ORIGINS") else ["*"]

app = FastAPI(
    title="BNERC Server",
    description="åŸºäº LangGraph çš„ Plan & Execute Agent æœåŠ¡",
    version="1.0.0",
    lifespan=lifespan  # æ·»åŠ ç”Ÿå‘½å‘¨æœŸç®¡ç†
)

# ä¸­é—´ä»¶
app.add_middleware(GZipMiddleware, minimum_size=1000)
app.add_middleware(
    CORSMiddleware,
    allow_origins=cors_origins,  # ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œç”Ÿäº§ç¯å¢ƒåº”é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],  # é™åˆ¶å…è®¸çš„æ–¹æ³•
    allow_headers=["*"],
    max_age=3600,  # é¢„æ£€è¯·æ±‚ç¼“å­˜æ—¶é—´
)

# è¯·æ±‚æ¨¡å‹
class RunRequest(BaseModel):
    assistant_id: str
    input: dict
    stream_mode: Optional[str] = "custom"

# ================================
# API è·¯ç”±
# ================================
@app.post("/threads")
async def create_thread():
    """åˆ›å»ºæ–°çš„å¯¹è¯çº¿ç¨‹"""
    thread_id = str(uuid.uuid4())
    logger.info(f"åˆ›å»ºæ–°çº¿ç¨‹: {thread_id}")
    return {"thread_id": thread_id}

@app.get("/threads/{thread_id}")
async def get_thread(thread_id: str):
    """è·å–çº¿ç¨‹çš„å†å²çŠ¶æ€ï¼ˆç”¨äºæ¢å¤å¯¹è¯ï¼‰"""
    try:
        global app_graph
        if app_graph is None:
            app_graph = await init_graph()
        
        config = {"configurable": {"thread_id": thread_id}}
        state = await app_graph.aget_state(config)
        
        if state and state.values:
            # æå–æ¶ˆæ¯å†å²
            messages = state.values.get("messages", [])
            message_list = []
            for msg in messages:
                if hasattr(msg, 'type') and hasattr(msg, 'content'):
                    message_list.append({
                        "role": "user" if msg.type == "human" else "assistant",
                        "content": extract_text(msg.content) if hasattr(msg, 'content') else str(msg.content)
                    })
            
            return {
                "thread_id": thread_id,
                "exists": True,
                "messages": message_list,
                "message_count": len(message_list)
            }
        else:
            return {
                "thread_id": thread_id,
                "exists": False,
                "messages": [],
                "message_count": 0
            }
    except Exception as e:
        logger.error(f"è·å–çº¿ç¨‹çŠ¶æ€å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–çº¿ç¨‹çŠ¶æ€å¤±è´¥: {str(e)}")

@app.post("/threads/{thread_id}/runs/stream")
async def run_stream(thread_id: str, run_request: RunRequest, request: Request):
    """æµå¼è¿è¡Œå¯¹è¯"""
    # ç”Ÿæˆè¯·æ±‚IDç”¨äºè¿½è¸ª
    request_id = str(uuid.uuid4())
    logger.info(f"[{request_id}] çº¿ç¨‹ {thread_id} å¼€å§‹æµå¼è¿è¡Œ")
    
    if run_request.assistant_id != "bridge_bindtools":
        logger.warning(f"[{request_id}] æ— æ•ˆçš„ assistant_id: {run_request.assistant_id}")
        raise HTTPException(status_code=400, detail="Invalid assistant_id")
    
    try:
        return StreamingResponse(
            stream_handler(run_request, thread_id, request_id),
            media_type="text/event-stream",
            headers={
                "X-Request-ID": request_id,
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            }
        )
    except Exception as e:
        logger.error(f"[{request_id}] æµå¼å¤„ç†å¯åŠ¨å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Stream initialization failed")

async def stream_handler(run_request: RunRequest, thread_id: str, request_id: str):
    """æµå¼å“åº”å¤„ç† - ä½¿ç”¨åå°ä»»åŠ¡ + é˜Ÿåˆ—"""
    queue = asyncio.Queue()
    
    async def run_graph_task():
        """åå°ä»»åŠ¡ï¼šæ‰§è¡Œå›¾å¹¶å°†äº‹ä»¶æ”¾å…¥é˜Ÿåˆ—"""
        try:
            global app_graph
            if app_graph is None:
                logger.info(f"[{request_id}] åˆå§‹åŒ– LangGraph å›¾...")
                app_graph = await init_graph()
            
            # æ„å»ºè¾“å…¥
            input_data = run_request.input
            messages = input_data.get("messages", [])
            files = input_data.get("files", [])
            
            # å°è¯•ä» checkpointer æ¢å¤å†å²çŠ¶æ€
            config = {"configurable": {"thread_id": thread_id}}
            existing_state = None
            
            try:
                # è·å–ç°æœ‰çŠ¶æ€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                existing_state = await app_graph.aget_state(config)
                if existing_state and existing_state.values:
                    logger.info(f"[{request_id}] æ¢å¤çº¿ç¨‹ {thread_id} çš„å†å²çŠ¶æ€ï¼Œå·²æœ‰ {len(existing_state.values.get('messages', []))} æ¡æ¶ˆæ¯")
                    # ä½¿ç”¨å†å²çŠ¶æ€ä¸­çš„ messages
                    existing_messages = existing_state.values.get("messages", [])
                    # åªæ·»åŠ æ–°çš„ç”¨æˆ·æ¶ˆæ¯ï¼ˆé¿å…é‡å¤ï¼‰
                    new_user_messages = [msg for msg in messages if msg["role"] == "user"]
                    for msg in new_user_messages:
                        existing_messages.append(HumanMessage(content=msg["content"]))
                    
                    initial_state = {
                        "messages": existing_messages,
                        "run_id": existing_state.values.get("run_id", str(uuid.uuid4())),
                        "files": files or existing_state.values.get("files", [])
                    }
                else:
                    # æ²¡æœ‰å†å²çŠ¶æ€ï¼Œåˆ›å»ºæ–°çš„
                    logger.info(f"[{request_id}] çº¿ç¨‹ {thread_id} æ˜¯æ–°å¯¹è¯ï¼Œåˆ›å»ºåˆå§‹çŠ¶æ€")
                    langchain_messages = []
                    for msg in messages:
                        if msg["role"] == "user":
                            langchain_messages.append(HumanMessage(content=msg["content"]))
                        elif msg["role"] == "assistant":
                            langchain_messages.append(AIMessage(content=msg["content"]))
                    
                    initial_state = {
                        "messages": langchain_messages,
                        "run_id": str(uuid.uuid4()),
                        "files": files
                    }
            except Exception as e:
                logger.warning(f"[{request_id}] æ¢å¤çŠ¶æ€å¤±è´¥: {e}ï¼Œä½¿ç”¨æ–°çŠ¶æ€")
                # æ¢å¤å¤±è´¥ï¼Œä½¿ç”¨æ–°çŠ¶æ€
                langchain_messages = []
                for msg in messages:
                    if msg["role"] == "user":
                        langchain_messages.append(HumanMessage(content=msg["content"]))
                    elif msg["role"] == "assistant":
                        langchain_messages.append(AIMessage(content=msg["content"]))
                
                initial_state = {
                    "messages": langchain_messages,
                    "run_id": str(uuid.uuid4()),
                    "files": files
                }
            
            logger.info(f"[{request_id}] å¼€å§‹æ‰§è¡Œ Plan & Execute å·¥ä½œæµ")
            
            # ä½¿ç”¨ astream æ‰§è¡Œå›¾
            async for event in app_graph.astream(
                initial_state,
                config,
                stream_mode=["custom", "updates"]
            ):
                if isinstance(event, tuple) and len(event) == 2:
                    mode, data = event
                    
                    if mode == "custom":
                        logger.debug(f"[{request_id}] [custom] {data}")
                        await queue.put(data)
                    elif mode == "updates" and data:
                        node_name = list(data.keys())[0]
                        logger.info(f"[{request_id}] [updates] èŠ‚ç‚¹ [{node_name}] å®Œæˆ")
                        
                        # åªå‘é€èŠ‚ç‚¹å®Œæˆäº‹ä»¶ï¼Œplan_final ç”± planner èŠ‚ç‚¹çš„ writer å‘é€
                        await queue.put({'event': 'node_complete', 'node': node_name})
                else:
                    if isinstance(event, dict):
                        await queue.put(event)
            
            logger.info(f"[{request_id}] å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
            
        except asyncio.CancelledError:
            logger.warning(f"[{request_id}] å›¾æ‰§è¡Œè¢«å–æ¶ˆ")
            await queue.put({'error': 'Cancelled'})
        except Exception as e:
            logger.error(f"[{request_id}] å›¾æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            await queue.put({'error': str(e)})
        finally:
            await queue.put(None)  # ç»“æŸæ ‡è®°
    
    # å¯åŠ¨åå°ä»»åŠ¡
    task = asyncio.create_task(run_graph_task())
    
    try:
        while True:
            try:
                data = await asyncio.wait_for(queue.get(), timeout=120.0)
                
                if data is None:
                    yield f"data: {json.dumps({'event': 'stream_end'})}\n\n"
                    break
                
                yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
                
            except asyncio.TimeoutError:
                logger.warning(f"[{request_id}] é˜Ÿåˆ—è¶…æ—¶")
                yield f"data: {json.dumps({'error': 'timeout', 'request_id': request_id})}\n\n"
                break
    except asyncio.CancelledError:
        logger.info(f"[{request_id}] æµå¼å¤„ç†è¢«å–æ¶ˆ")
        yield f"data: {json.dumps({'error': 'cancelled', 'request_id': request_id})}\n\n"
    except Exception as e:
        logger.error(f"[{request_id}] æµå¼å¤„ç†å¤±è´¥: {e}", exc_info=True)
        yield f"data: {json.dumps({'error': str(e), 'request_id': request_id})}\n\n"
    finally:
        if not task.done():
            task.cancel()
            try:
                await asyncio.wait_for(task, timeout=5.0)  # ç­‰å¾…æœ€å¤š5ç§’
            except (asyncio.CancelledError, asyncio.TimeoutError):
                logger.warning(f"[{request_id}] åå°ä»»åŠ¡æ¸…ç†è¶…æ—¶")
                pass

@app.get("/threads")
async def list_threads(limit: int = 50, offset: int = 0):
    """åˆ—å‡ºæ‰€æœ‰çº¿ç¨‹ï¼ˆä»…æ”¯æŒ PostgreSQLï¼‰"""
    try:
        global checkpointer
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ PostgreSQL
        is_postgres = False
        if checkpointer is not None:
            checkpointer_class_name = type(checkpointer).__name__
            is_postgres = "Postgres" in checkpointer_class_name
        
        if not is_postgres:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ MemorySaver
            from langgraph.checkpoint.memory import MemorySaver
            if isinstance(checkpointer, MemorySaver):
                return {
                    "error": "å†…å­˜å­˜å‚¨ä¸æ”¯æŒåˆ—å‡ºæ‰€æœ‰çº¿ç¨‹",
                    "hint": "è¯·é…ç½® PostgreSQL ä»¥æŸ¥çœ‹æ‰€æœ‰å†å²çº¿ç¨‹"
                }
        
        # å¦‚æœä½¿ç”¨ PostgreSQLï¼Œç›´æ¥æŸ¥è¯¢æ•°æ®åº“
        db_conn_string = os.getenv("DB_CONN_STRING", "")
        if db_conn_string and POSTGRES_AVAILABLE:
            import asyncpg
            from urllib.parse import urlparse
            
            # è§£æè¿æ¥å­—ç¬¦ä¸²
            parsed = urlparse(db_conn_string.replace("postgresql+asyncpg://", "postgresql://"))
            if parsed.scheme == "":
                parsed = urlparse("postgresql://" + db_conn_string)
            
            conn = await asyncpg.connect(
                host=parsed.hostname or "localhost",
                port=parsed.port or 5432,
                user=parsed.username or "postgres",
                password=parsed.password or "",
                database=parsed.path.lstrip("/") if parsed.path else "postgres"
            )
            
            try:
                # æŸ¥è¯¢æ‰€æœ‰å”¯ä¸€çš„ thread_id
                query = """
                    SELECT DISTINCT thread_id, 
                           MAX(checkpoint_id) as latest_checkpoint,
                           COUNT(*) as checkpoint_count
                    FROM checkpoints
                    GROUP BY thread_id
                    ORDER BY latest_checkpoint DESC
                    LIMIT $1 OFFSET $2
                """
                rows = await conn.fetch(query, limit, offset)
                
                # æŸ¥è¯¢æ€»æ•°
                count_query = "SELECT COUNT(DISTINCT thread_id) as total FROM checkpoints"
                total_row = await conn.fetchrow(count_query)
                total = total_row['total'] if total_row else 0
                
                threads = []
                for row in rows:
                    threads.append({
                        "thread_id": row['thread_id'],
                        "checkpoint_count": row['checkpoint_count'],
                        "latest_checkpoint": row['latest_checkpoint']
                    })
                
                return {
                    "threads": threads,
                    "total": total,
                    "limit": limit,
                    "offset": offset
                }
            finally:
                await conn.close()
        else:
            return {
                "error": "PostgreSQL æœªé…ç½®",
                "hint": "è¯·è®¾ç½® USE_POSTGRES=true å’Œ DB_CONN_STRING ç¯å¢ƒå˜é‡"
            }
    except Exception as e:
        logger.error(f"åˆ—å‡ºçº¿ç¨‹å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"åˆ—å‡ºçº¿ç¨‹å¤±è´¥: {str(e)}")

@app.get("/threads/{thread_id}/history")
async def get_thread_history(thread_id: str, limit: int = 100):
    """è·å–çº¿ç¨‹çš„å®Œæ•´å†å²ï¼ˆåŒ…æ‹¬æ‰€æœ‰ checkpointsï¼‰"""
    try:
        global app_graph
        if app_graph is None:
            app_graph = await init_graph()
        
        config = {"configurable": {"thread_id": thread_id}}
        
        # è·å–å½“å‰çŠ¶æ€
        state = await app_graph.aget_state(config)
        
        if not state or not state.values:
            raise HTTPException(status_code=404, detail="Thread not found")
        
        # æå–å®Œæ•´ä¿¡æ¯
        messages = state.values.get("messages", [])
        message_list = []
        for idx, msg in enumerate(messages):
            msg_info = {
                "index": idx,
                "type": getattr(msg, 'type', 'unknown'),
                "content": extract_text(msg.content) if hasattr(msg, 'content') else str(msg.content),
            }
            
            # æ·»åŠ é¢å¤–ä¿¡æ¯
            if hasattr(msg, 'tool_calls') and msg.tool_calls:
                msg_info["tool_calls"] = [
                    {
                        "name": tc.get("name", ""),
                        "args": tc.get("args", {})
                    } for tc in msg.tool_calls
                ]
            
            if hasattr(msg, 'tool_call_id'):
                msg_info["tool_call_id"] = msg.tool_call_id
            
            message_list.append(msg_info)
        
        # è·å–å…¶ä»–çŠ¶æ€ä¿¡æ¯
        state_info = {
            "thread_id": thread_id,
            "run_id": state.values.get("run_id"),
            "plan": state.values.get("plan", []),
            "current_step": state.values.get("current_step", 0),
            "files": state.values.get("files", []),
            "message_count": len(message_list),
            "messages": message_list[:limit]  # é™åˆ¶è¿”å›æ•°é‡
        }
        
        return state_info
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–çº¿ç¨‹å†å²å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–çº¿ç¨‹å†å²å¤±è´¥: {str(e)}")

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ - ç”Ÿäº§ç¯å¢ƒåº”æ£€æŸ¥å…³é”®ä¾èµ–"""
    import time
    global checkpointer, db_pool
    
    # æ£€æŸ¥ checkpointer ç±»å‹
    checkpointer_type = "æœªåˆå§‹åŒ–"
    checkpointer_status = "unknown"
    is_postgres = False
    db_connection_status = "unknown"
    checkpoint_count = 0
    thread_count = 0
    
    if checkpointer is not None:
        checkpointer_type = type(checkpointer).__name__
        is_postgres = "Postgres" in checkpointer_type or "PostgresSaver" in checkpointer_type
        
        if is_postgres:
            checkpointer_status = "active"
            # æ£€æŸ¥è¿æ¥æ± çŠ¶æ€
            if db_pool is not None:
                try:
                    # æ£€æŸ¥è¿æ¥æ± æ˜¯å¦æ‰“å¼€
                    if hasattr(db_pool, '_pool') and db_pool._pool:
                        db_connection_status = "connected"
                    else:
                        db_connection_status = "pool_not_ready"
                except Exception:
                    db_connection_status = "unknown"
            
            # å°è¯•æŸ¥è¯¢æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
            try:
                db_conn_string = os.getenv("DB_CONN_STRING", "")
                if db_conn_string:
                    from psycopg import AsyncConnection
                    conninfo = db_conn_string.replace("postgresql+asyncpg://", "postgresql://")
                    async with await AsyncConnection.connect(conninfo) as conn:
                        async with conn.cursor() as cur:
                            # æŸ¥è¯¢ checkpoint æ•°é‡
                            await cur.execute("SELECT COUNT(*) FROM checkpoints")
                            result = await cur.fetchone()
                            checkpoint_count = result[0] if result else 0
                            
                            # æŸ¥è¯¢ thread æ•°é‡
                            await cur.execute("SELECT COUNT(DISTINCT thread_id) FROM checkpoints")
                            result = await cur.fetchone()
                            thread_count = result[0] if result else 0
            except Exception as e:
                logger.debug(f"æŸ¥è¯¢æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        else:
            from langgraph.checkpoint.memory import MemorySaver
            if isinstance(checkpointer, MemorySaver):
                checkpointer_status = "memory_only"
            else:
                checkpointer_status = "active"
    else:
        checkpointer_status = "not_initialized"
    
    health_status = {
        "status": "healthy",
        "timestamp": time.time(),
        "service": "bnerc_server",
        "version": "1.0.0",
        "checkpointer": {
            "type": checkpointer_type,
            "status": checkpointer_status,
            "is_postgres": is_postgres,
        "postgres_available": POSTGRES_AVAILABLE
        },
        "database": {
            "connection_status": db_connection_status,
            "checkpoint_count": checkpoint_count,
            "thread_count": thread_count
        }
    }
    
    # æ£€æŸ¥å…³é”®ç»„ä»¶ï¼ˆå¯é€‰ï¼‰
    try:
        # æ£€æŸ¥å›¾æ˜¯å¦å·²åˆå§‹åŒ–ï¼ˆå¦‚æœæœªåˆå§‹åŒ–ï¼Œå»¶è¿Ÿåˆå§‹åŒ–ä¹Ÿå¯ä»¥ï¼‰
        if app_graph is None:
            health_status["graph_initialized"] = False
        else:
            health_status["graph_initialized"] = True
        
        # å¯ä»¥æ·»åŠ æ›´å¤šå¥åº·æ£€æŸ¥ï¼Œå¦‚æ•°æ®åº“è¿æ¥ã€MCP è¿æ¥ç­‰
        return health_status
    except Exception as e:
        logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        raise HTTPException(status_code=503, detail="Service unhealthy")

# ================================
# ASGI å…¥å£ç‚¹
# ================================
asgi_app = app

# ================================
# ä¸»å‡½æ•°
# ================================
if __name__ == "__main__":
    import uvicorn
    import signal
    import sys
    
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "2026"))
    workers = int(os.getenv("WORKERS", "1"))  # å·¥ä½œè¿›ç¨‹æ•°
    timeout_keep_alive = int(os.getenv("TIMEOUT_KEEP_ALIVE", "60"))  # ä¿æŒè¿æ¥è¶…æ—¶
    timeout_graceful_shutdown = int(os.getenv("TIMEOUT_GRACEFUL_SHUTDOWN", "30"))  # ä¼˜é›…å…³é—­è¶…æ—¶
    
    logger.info(f"å¯åŠ¨ BNERC Server: http://{host}:{port}")
    logger.info(f"å·¥ä½œè¿›ç¨‹æ•°: {workers}")
    logger.info(f"ä¿æŒè¿æ¥è¶…æ—¶: {timeout_keep_alive}s")
    logger.info(f"ä¼˜é›…å…³é—­è¶…æ—¶: {timeout_graceful_shutdown}s")
    
    # ç”Ÿäº§ç¯å¢ƒé…ç½®
    uvicorn_config = {
        "app": "bnerc_server:asgi_app",
        "host": host,
        "port": port,
        "reload": False,  # ç”Ÿäº§ç¯å¢ƒç¦ç”¨è‡ªåŠ¨é‡è½½
        "workers": workers,
        "log_level": "info",
        "access_log": True,
        "timeout_keep_alive": timeout_keep_alive,
        "timeout_graceful_shutdown": timeout_graceful_shutdown,
        "limit_max_requests": 100000,  # æ¯ä¸ªå·¥ä½œè¿›ç¨‹å¤„ç†çš„æœ€å¤§è¯·æ±‚æ•°
        "limit_concurrency": None,  # æœ€å¤§å¹¶å‘æ•°ï¼ˆNone è¡¨ç¤ºæ— é™åˆ¶ï¼Œç”Ÿäº§ç¯å¢ƒå¯è®¾ç½®ï¼‰
        "backlog": 2048,  # è¿æ¥é˜Ÿåˆ—å¤§å°
    }
    
    try:
        uvicorn.run(**uvicorn_config)
    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°é”®ç›˜ä¸­æ–­ï¼Œé€€å‡ºæœåŠ¡")
    except Exception as e:
        logger.error(f"æœåŠ¡å¯åŠ¨å¤±è´¥: {e}", exc_info=True)
        sys.exit(1)
