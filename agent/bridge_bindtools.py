import json
from typing import Dict, List, Any
from langchain_core.messages import AIMessage, SystemMessage, HumanMessage, ToolMessage
from langgraph.prebuilt import ToolNode
# import sys
# sys.path.append('/root/langgraph')
from utils.file import load_prompt_from_yaml
# å¯¼å…¥æ—¶é—´æ¨¡å—ç”¨äºç¼“å­˜
import time
from langgraph.graph import StateGraph, END, START,MessagesState
from langgraph.types import StreamWriter
from tools.tools import TOOLS
from typing import Any, Optional,Union
from typing_extensions import TypedDict
from utils.draw import draw_workflow
import logging
import asyncio
import uuid
from dotenv import load_dotenv
load_dotenv()
# é…ç½®loggingåŸºæœ¬è®¾ç½®
logging.basicConfig(
    level=logging.INFO,  # è®¾ç½®æ—¥å¿—çº§åˆ«ä¸ºINFO
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',  # æ—¥å¿—æ ¼å¼
    handlers=[
        logging.StreamHandler()  # è¾“å‡ºåˆ°æ§åˆ¶å°çš„å¤„ç†å™¨
    ]
)
logger = logging.getLogger(__name__)

# ================================
# 2. LLM å®šä¹‰
# ================================
# ä»utilså¯¼å…¥å…¨å±€LLMå®ä¾‹
from utils.utils import get_llm_instance
from agent.mcp.mcp_client import client

llm = get_llm_instance()
# åˆ›å»ºä¸€ä¸ªå…¨å±€å˜é‡æ¥å­˜å‚¨ç»‘å®šäº†å·¥å…·çš„LLMå®ä¾‹
llm_with_tools = None
# å·¥å…·å‡½æ•°å·²ç§»è‡³tools/tools.pyæ–‡ä»¶ä¸­
# æ³¨æ„ï¼šåœ¨LangGraph APIç¯å¢ƒä¸­ï¼Œè‡ªå®šä¹‰checkpointerä¼šè¢«å¿½ç•¥ï¼Œå¹³å°ä¼šè‡ªåŠ¨å¤„ç†æŒä¹…æ€§
# MCPå·¥å…·åˆ—è¡¨ç¼“å­˜
mcp_tools_cache = None
cache_timestamp = 0
cache_duration = 60  # ç¼“å­˜æœ‰æ•ˆæœŸï¼Œå•ä½ï¼šç§’

# ================================
# 3. State å®šä¹‰
# ================================

class State(MessagesState):
    # messages: Union[List[BaseMessage], List[dict]]          # å¯¹è¯ / ä»»åŠ¡è¿›åº¦
    run_id: str
    plan: List[Dict[str, Any]]              # Planner ç”Ÿæˆçš„è®¡åˆ’
    current_step: int                          # å½“å‰æ‰§è¡Œåˆ°ç¬¬å‡ æ­¥ 
    current_tool: Optional[Dict[str, Any]]     # å½“å‰è¦è°ƒç”¨çš„å·¥å…·
    files: List[Dict[str, Any]]   # â† æ–‡ä»¶å…ƒä¿¡æ¯
    result: Dict[str, Any]              # å·¥å…·æ‰§è¡Œç»“æœ
    replan: bool                            # æ˜¯å¦éœ€è¦é‡æ–°è§„åˆ’
    reply: Optional[str]                    # æœ€ç»ˆå›å¤

# ================================
# 4. åŠ è½½æç¤ºè¯æ¨¡æ¿
# ================================

# åŠ è½½æç¤ºè¯æ¨¡æ¿
planner_prompt_data = load_prompt_from_yaml('planner2.yaml')
# router_prompt_data = load_prompt_from_yaml('router.yaml')
checker_prompt_data = load_prompt_from_yaml('checker.yaml')
summarizer_prompt_data = load_prompt_from_yaml('summarizer.yaml')

# ====================================
# 1. Plannerï¼ˆç”Ÿæˆå·¥å…·æ‰§è¡Œè®¡åˆ’ï¼‰
# ====================================

async def safe_get_tools() -> list:
    try:
        return await client.get_tools()
    except Exception as e:
        logger.warning(f"MCP è¿æ¥å¤±è´¥ï¼Œä½¿ç”¨ç©ºå·¥å…·åˆ—è¡¨é™çº§: {e}")
        return []

# å¼‚æ­¥åˆå§‹åŒ–å‡½æ•°ï¼Œç”¨äºç»‘å®šå·¥å…·åˆ°LLM
async def initialize_llm_with_tools():
    global llm_with_tools
    if llm_with_tools is None:
        mcp_tools = await safe_get_tools()
        llm_with_tools = llm.bind_tools(mcp_tools)
        logger.info("LLMå·²æˆåŠŸç»‘å®šå·¥å…·")
    return llm_with_tools
# åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„å·¥å…·åˆ—è¡¨ï¼Œç¨ååœ¨åº”ç”¨å¯åŠ¨æ—¶å¡«å……
tools = []
# å¼‚æ­¥è·å–å·¥å…·åˆ—è¡¨
async def get_tools_for_toolnode():
    return await safe_get_tools()

# ä¸èƒ½ç›´æ¥ await client.get_tools()
def get_result_key(tool_name, existing_results):
    """è·å–å·¥å…·æ‰§è¡Œç»“æœçš„é”®åï¼Œå¤„ç†é‡å¤è°ƒç”¨æƒ…å†µ"""
    # å¤„ç†å¤šæ¬¡è°ƒç”¨åŒä¸€å·¥å…·çš„æƒ…å†µï¼Œä¸ºç»“æœæ·»åŠ æ­¥éª¤ç´¢å¼•
    tool_index = 1
    result_key = tool_name
    
    # å¦‚æœå·²å­˜åœ¨è¯¥å·¥å…·çš„ç»“æœï¼Œæ·»åŠ ç´¢å¼•
    while result_key in existing_results:
        tool_index += 1
        result_key = f"{tool_name}_{tool_index}"
    
    return result_key

def extract_tool_results(messages):
    results = []
    for m in messages:
        if isinstance(m, ToolMessage):
            results.append({
                "tool_call_id": m.tool_call_id,
                "content": m.content,
            })
    return results

# conn   = aiosqlite.connect("checkpoints.db", isolation_level=None)
# saver  = SqliteSaver(conn)

def extract_text(content) -> str:
    if isinstance(content, str):
        return content
    if isinstance(content, list):
        # å¸¸è§æ ¼å¼ï¼š[{type: "text", text: "..."}]
        texts = []
        for item in content:
            if isinstance(item, dict) and item.get("type") == "text":
                texts.append(item.get("text", ""))
        return "\n".join(texts)
    return str(content)

def parse_tool_result(content) -> bool:
    """
    åˆ¤æ–­å·¥å…·æ˜¯å¦æ‰§è¡ŒæˆåŠŸ
    å…¼å®¹ ToolMessage.content çš„æ‰€æœ‰å¸¸è§ç»“æ„
    """
    try:
        # ToolMessage.content æ˜¯ listï¼ˆLangChain æ–°ç»“æ„ï¼‰
        if isinstance(content, list) and content:
            block = content[0]
            if isinstance(block, dict) and "text" in block:
                raw = block["text"]
            else:
                return False

        # ç›´æ¥æ˜¯å­—ç¬¦ä¸²
        elif isinstance(content, str):
            raw = content

        # ç›´æ¥æ˜¯ dict
        elif isinstance(content, dict):
            return content.get("status") == "success"

        else:
            return False

        # è§£æ JSON å­—ç¬¦ä¸²
        data = json.loads(raw)
        return data.get("status") == "success"

    except Exception as e:
        logger.error(f"parse_tool_result failed: {e}")
        return False

def get_recent_run_tool_messages(messages, run_id):
    results = []
    message_count = len(messages)
    i = message_count - 1

    # ä»å°¾éƒ¨å‘å‰æ‰«æï¼ŒæŸ¥æ‰¾åŒ¹é…çš„ ToolMessage
    while i >= 0:
        m = messages[i]
        if isinstance(m, ToolMessage):
            # æ£€æŸ¥å‰ä¸€æ¡æ¶ˆæ¯æ˜¯å¦ä¸º AIMessageï¼ˆå·¥å…·è°ƒç”¨ï¼‰ï¼Œå¹¶éªŒè¯ run_id
            if i >= 1:
                prev_msg = messages[i-1]
                if isinstance(prev_msg, AIMessage) and prev_msg.additional_kwargs.get("run_id") == run_id:
                    results.append(m)
                else:
                    # é‡åˆ°ä¸å±äºæœ¬æ¬¡ run çš„ ToolMessageï¼Œç»ˆæ­¢æ‰«æ
                    break
            else:
                # ToolMessage æ²¡æœ‰å‰ä¸€æ¡æ¶ˆæ¯ï¼Œæ— æ³•éªŒè¯ run_idï¼Œç»ˆæ­¢æ‰«æ
                break
            i -= 1
        elif isinstance(m, AIMessage) and m.additional_kwargs.get("run_id") == run_id:
            # ç»§ç»­å‘å‰æ‰«æï¼Œå¯èƒ½è¿˜æœ‰æ›´å¤šçš„ ToolMessage
            i -= 1
        else:
            # é‡åˆ°ä¸å±äºæœ¬æ¬¡ run çš„é ToolMessageï¼Œç»ˆæ­¢æ‰«æ
            break

    return list(reversed(results))

def init_state():
    return {
        "messages": [],
        "run_id": str(uuid.uuid4()),
        "plan": [],
        "current_step": 0
    }

async def planner(state: State, writer: StreamWriter):
    """ç”Ÿæˆæ‰§è¡Œè®¡åˆ’ï¼Œå†™å…¥ plan å’Œåˆå§‹åŒ–çŠ¶æ€"""
    messages = state.get("messages", [])
    run_id = state.get("run_id", str(uuid.uuid4()))
    logger.info("messages:" + str(messages))
    # è·å–ç”¨æˆ·è¾“å…¥
    # user_input = messages[-1]['content'] if messages else ""
    # logger.info("ç”¨æˆ·è¾“å…¥:" + user_input)
    conversation = "\n".join(
        f"{m.type}: {m.content}" for m in messages[:-1] 
        if m.type in ['human', 'ai'] and hasattr(m, 'content') and m.content.strip()  # åªåŒ…å«humanå’Œaiç±»å‹ä¸”å†…å®¹éç©º
    )
    logger.info("å†å²å¯¹è¯:\n" + conversation)
    user_input = ""
    files = []

    if messages and isinstance(messages[-1], HumanMessage):
        # æå–æ–‡æœ¬å†…å®¹
        user_input = extract_text(messages[-1].content)
    
    # å¤„ç†stateä¸­çš„æ–‡ä»¶ï¼ˆä»ç‹¬ç«‹çš„fileså­—æ®µæå–ï¼‰
    state_files = state.get("files", [])
    files.extend(state_files)
    
    logger.info(f"ç”¨æˆ·è¾“å…¥: {user_input}")
    if files:
        for f in files:
            logger.info(f"æ”¶åˆ°æ–‡ä»¶: {f.get('name')} (è·¯å¾„: {f.get('path')})")
    # ä½¿ç”¨ä»YAMLåŠ è½½çš„æç¤ºè¯ï¼Œä¼ å…¥åˆå¹¶åçš„å·¥å…·æè¿°
    system_prompt = planner_prompt_data['system'].format(user_input=user_input, conversation=conversation, files=files)
    # å¿…æœ‰çš„writer
    writer({"event": "plan_start", "text": "ğŸ¯ æ­£åœ¨ç”Ÿæˆè®¡åˆ’...\n"})
    # ç¡®ä¿LLMå·²ç»‘å®šå·¥å…·
    global llm_with_tools
    if llm_with_tools is None:
        await initialize_llm_with_tools()

    # ä½¿ç”¨ç»‘å®šäº†å·¥å…·çš„LLMè°ƒç”¨ç”Ÿæˆè®¡åˆ’
    response = await llm_with_tools.ainvoke(
            [
                SystemMessage(content=system_prompt)
            ]
        )
    
    try:
        # è§£æå“åº”ä¸ºJSON
        response_content = response.content if hasattr(response, 'content') else str(response)
        # logger.info(response_content)
        parsed_response = json.loads(response_content)
        logger.info(parsed_response)
        
        # éªŒè¯å“åº”æ ¼å¼æ˜¯å¦æ­£ç¡®
        if not isinstance(parsed_response, dict) or "plan" not in parsed_response:
            error_message = "è®¡åˆ’æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘å¿…è¦å­—æ®µ"
            return {
            "plan": [],
            "current_step": 0,
            "messages": messages + [AIMessage(content=error_message)],
            "run_id": run_id  # ä¿ç•™æˆ–ç”Ÿæˆrun_id
        }
        # logger.info(f"Planner: {parsed_response['plan']}")

        # ç»˜åˆ¶å·¥ä½œæµç¨‹å›¾å¹¶ä¿å­˜åˆ°workflowç›®å½•
        draw_workflow(parsed_response["plan"])
        # å¤„ç†æ‰§è¡Œè®¡åˆ’ï¼Œç¡®ä¿åªåŒ…å«æœ‰æ•ˆçš„toolå­—æ®µ
        processed_plan = []
        for step in parsed_response["plan"]:
            tool_name = step.get("tool", "")
            if tool_name:  # åªä¿ç•™æœ‰æœ‰æ•ˆtoolåç§°çš„æ­¥éª¤
                processed_step = {"tool": tool_name}
                processed_plan.append(processed_step)
        logger.info(f"Planner: å¤„ç†åçš„æ‰§è¡Œè®¡åˆ’: {processed_plan}")
        if writer:
            writer({"event": "plan_final", "plan": processed_plan})   # åªæ¨æˆå“
            writer({"event": "plan_end", "text": "\n"})
        # è¿”å›æ›´æ–°åçš„çŠ¶æ€
        # è®¡åˆ’æ›´æ–°åˆ°Stateé‡Œ
        # èµ„æºè·¯å¾„ä»¥é”®å€¼å¯¹å½¢å¼åŠ å…¥Stateçš„resulté‡Œ
        # messages.append(AIMessage(content=response_content))
        # logger.info(f"Planner: æœ€åçš„messages: {messages}")
        logger.info(f"Planner run_id: {run_id}")
        return {
            "plan": processed_plan,
            "current_step": 0,
            "messages": messages,
            "files": files,  # å°†æ–‡ä»¶ä¿¡æ¯ä¿å­˜åˆ°stateä¸­
            "run_id": run_id  # ä¿ç•™æˆ–ç”Ÿæˆrun_id
        }
    except json.JSONDecodeError:
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é”™è¯¯æ¶ˆæ¯
        error_message = "è®¡åˆ’ç”Ÿæˆå¤±è´¥ï¼Œè¯·é‡è¯•"
        return {
            "plan": [],
            "current_step": 0,
            "messages": messages + [
                # {"role": "assistant", "content": error_message}
                AIMessage(content=error_message)
            ],
            "run_id": run_id  # ä¿ç•™æˆ–ç”Ÿæˆrun_id
        }
    except Exception as e:
        # æ•è·å…¶ä»–å¼‚å¸¸
        error_message = f"è®¡åˆ’ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        return {
            "plan": [],
            "current_step": 0,
            "messages": messages + [
                # {"role": "assistant", "content": error_message}
                AIMessage(content=error_message)
            ],
        }

# ====================================
# 2. Executorï¼ˆå†³å®šå½“å‰æ‰§è¡Œå“ªä¸ªæ­¥éª¤ï¼‰
# ====================================

async def executor(state: State):
    """
    Executorï¼š
    - åˆ¤æ–­è®¡åˆ’æ˜¯å¦ç»“æŸ
    - è‹¥æœªç»“æŸï¼šä½¿ç”¨ bind_tools çš„ LLM ä¸ºå½“å‰å·¥å…·ç”Ÿæˆ tool_call
    - äº¤ç»™ ToolNode æ‰§è¡Œ
    """

    plan = state.get("plan", [])
    current_step = state.get("current_step", 0)
    messages = state.get("messages", [])

    # ---------- ç©ºè®¡åˆ’ ----------
    if not plan:
        return {
            "next": "summarizer",
            "plan": plan,
            "messages": messages,
        }
        
    success = True
    result_json = None

    # -----------------------------
    # 1. è§£æä¸Šä¸€ä¸ªå·¥å…·æ‰§è¡Œç»“æœ
    # -----------------------------
    if messages and isinstance(messages[-1], ToolMessage):
        last_tool_msg = messages[-1]
        try:
            raw_text = last_tool_msg.content[0]["text"]
            result_json = json.loads(raw_text)
            success = result_json.get("status") == "success"
        except Exception as e:
            logger.error(f"Executor: è§£æå·¥å…·ç»“æœå¤±è´¥: {e}")
            success = False

    # -----------------------------
    # 2. å¤±è´¥å¤„ç† + é‡è¯•
    # -----------------------------
    retry_count = state.get("retry_count", 0)
    max_retry = 1

    if not success:
        logger.error(
            f"Executor: å·¥å…·æ‰§è¡Œå¤±è´¥: "
            f"{result_json.get('message') if isinstance(result_json, dict) else ''}"
        )

        if retry_count < max_retry:
            logger.info(
                f"Executor: å·¥å…·æ‰§è¡Œå¤±è´¥ï¼Œæ­£åœ¨é‡è¯• "
                f"({retry_count + 1}/{max_retry})"
            )

            return {
                "messages": messages[:-1],   # å›æ»š tool_call + tool_result
                "current_step": current_step,  # âš ï¸ ä¸å‰è¿›
                "plan": plan,
                "retry_count": retry_count + 1,
            }
    
    # è®¡åˆ’æ‰§è¡Œå®Œï¼Œè¿›å…¥ checker
    if current_step >= len(plan):
        return {
            "current_step": current_step,
            "next": "check",
            "messages": messages,
        }

    # ---------- å½“å‰æ­¥éª¤ ----------
    step = plan[current_step]
    tool_name = step["tool"]

    logger.info(
        f"Executor: Step {current_step + 1}/{len(plan)} â†’ {tool_name}"
    )
    # åœ¨æ­¤ä¼ é€’äº†æ–‡ä»¶ä¿¡æ¯
    files = state.get("files", [])
    file_context = ""
    if files:
        file_context = (
            "å½“å‰å¯ç”¨æ–‡ä»¶å¦‚ä¸‹ï¼š\n" +
            "\n".join(
                f"- {f['name']} ({f['path']}, {f.get('type')})"
                for f in files
            )
        )
    # ---------- è®© LLM ç”Ÿæˆ tool_call ----------
    llm_messages = messages + [
        SystemMessage(content=file_context),
        HumanMessage(
            content=f"""
            è¯·è°ƒç”¨å·¥å…· `{tool_name}`ã€‚
            è¦æ±‚ï¼š
            - è‡ªåŠ¨ä»ä¸Šä¸‹æ–‡ä¸­æå–æ‰€éœ€å‚æ•°
            - ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæ–‡æœ¬
            """
        )
    ]

    if llm_with_tools is None:
        await initialize_llm_with_tools()

    response = await llm_with_tools.ainvoke(
        llm_messages,
        tool_choice={
            "type": "function",
            "function": {"name": tool_name},
        },
    )

    if not response.tool_calls:
        raise RuntimeError(
            f"Executor: LLM æœªç”Ÿæˆ tool_call,{tool_name}"
        )
    logger.info(f"Executor run_id: {state['run_id']}")
    # ç»™ç”¨äºç”Ÿæˆtool_callçš„AIMessageåŠ ä¸Š run_id
    response.additional_kwargs["run_id"] = state["run_id"]
    logger.info(state["run_id"])
    # responseæ˜¯AIMessageï¼ŒåŒ…å«tool_calls
    return {
        "messages": messages + [response],
        "current_step": current_step + 1,  # é€’å¢æ­¥éª¤ï¼Œé¿å…æ— é™å¾ªç¯
        "plan": plan,
        "run_id": state["run_id"],
        "next": "tools",   # â† äº¤ç»™ ToolNode
        "retry_count": 0
    }

# ====================================
# 4. Checkerï¼ˆæ£€æŸ¥æ˜¯å¦æ»¡è¶³ç”¨æˆ·éœ€æ±‚ï¼‰
# ====================================
async def checker(state: State):
    """æ£€æŸ¥æœ€ç»ˆæ‰§è¡Œç»“æœï¼Œä½¿ç”¨LLMè¯„ä¼°æ˜¯å¦éœ€è¦é‡è§„åˆ’"""
    # è·å–æœ€æ–°çš„ç”¨æˆ·æŸ¥è¯¢ï¼ˆä»messagesä¸­æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼‰
    messages = state["messages"]
    # logger.info(f"Checker: æ¶ˆæ¯çŠ¶æ€ - {messages}")
    user_query = None
    for m in reversed(messages):
        if isinstance(m, HumanMessage):
            user_query = m.content
            break

    # 2. æå–å·¥å…·æ‰§è¡Œç»“æœï¼ˆä»…æœ¬æ¬¡å¯¹è¯ï¼‰
    recent_tool_messages = get_recent_run_tool_messages(messages, state["run_id"])
    tool_results = extract_tool_results(recent_tool_messages)
    if not tool_results:
        final_result = ""
    else:
        final_result = json.dumps(tool_results, ensure_ascii=False, indent=2)
    logger.info(f"Checker: æå–åˆ°çš„ - {final_result}")

    conversation = "\n".join(
        f"{m.type}: {m.content}" for m in messages[:-1] 
        if m.type in ['human', 'ai'] and hasattr(m, 'content') and m.content.strip()  # åªåŒ…å«humanå’Œaiç±»å‹ä¸”å†…å®¹éç©º
    )

    logger.info("\nChecker: è¯„ä¼°æ‰§è¡Œç»“æœæ˜¯å¦æ»¡è¶³éœ€æ±‚")
    logger.info(f"Checker: ç”¨æˆ·é—®é¢˜: {user_query}")
    logger.info(f"Checker: æ‰§è¡Œç»“æœ: {final_result}")

    # ä½¿ç”¨LLMè¯„ä¼°æ‰§è¡Œç»“æœæ˜¯å¦æ»¡è¶³ç”¨æˆ·éœ€æ±‚
    # ä½¿ç”¨ä»YAMLåŠ è½½çš„æç¤ºè¯
    system_prompt = checker_prompt_data['system']
    
    # æ„å»ºå®Œæ•´çš„å¯¹è¯å†å²ä½œä¸ºä¸Šä¸‹æ–‡
    evaluation_prompt = [
        SystemMessage(content=system_prompt.format(
            user_query=user_query,
            final_result=final_result,
            conversation=conversation
        ))
    ]
    
    try:
        # ä½¿ç”¨å¼‚æ­¥æ–¹å¼è°ƒç”¨llmï¼Œé¿å…é˜»å¡
        import asyncio
        if hasattr(llm, "ainvoke"):
            response = await llm.ainvoke(evaluation_prompt)
        else:
            response = await asyncio.to_thread(llm.invoke, evaluation_prompt)
        
        evaluation_result = json.loads(response.content.strip())
        
        logger.info(f"Checker: è¯„ä¼°ç»“æœ - æ»¡è¶³éœ€æ±‚: {evaluation_result['satisfies_needs']}, éœ€è¦é‡æ–°è§„åˆ’: {evaluation_result['needs_replan']}")
        logger.info(f"Checker: è¯„ä¼°ç†ç”±: {evaluation_result['reason']}")
        # æš‚ä¸é‡è§„åˆ’
        evaluation_result["needs_replan"]=False
        # å¦‚æœéœ€è¦é‡æ–°è§„åˆ’ï¼Œå°†å½“å‰æ‰§è¡Œç»“æœæ·»åŠ åˆ°messagesä¸­ï¼Œç”¨äºé‡æ–°è§„åˆ’
        if evaluation_result["needs_replan"]:
            state["messages"].append({
                "role": "assistant",
                "content": f"å½“å‰æ‰§è¡Œç»“æœä¸æ»¡è¶³éœ€æ±‚: {evaluation_result['reason']}\n\nå½“å‰æ‰§è¡Œç»“æœ: {final_result}"
            })
            
            return {
                "replan": True,
                "next": "replan",  # ä¿®æ”¹ä¸ºä¸æ¡ä»¶è¾¹é…ç½®åŒ¹é…çš„å€¼
                "plan": state.get("plan", []),  # ä¿ç•™æ‰§è¡Œè®¡åˆ’
                "messages": state.get("messages", [])  # ä¿ç•™å¯¹è¯å†å²
            }
        else:
            # æ£€æŸ¥é€šè¿‡ï¼Œå°†è¿›å…¥summarizerèŠ‚ç‚¹ç”Ÿæˆæœ€ç»ˆå›å¤
            if not evaluation_result["satisfies_needs"]:
                # å¦‚æœä¸æ»¡è¶³éœ€æ±‚ä½†ä¹Ÿä¸é‡æ–°è§„åˆ’ï¼Œé¢„å…ˆè®¾ç½®ä¸€ä¸ªåŸºæœ¬å›å¤
                state["reply"] = final_result
            return {
                "replan": False,
                "next": "end",
                "plan": state.get("plan", []),  # ä¿ç•™æ‰§è¡Œè®¡åˆ’
                "messages": state.get("messages", [])  # ä¿ç•™å¯¹è¯å†å²
            }
    except Exception as e:
        logger.info(f"Checker: è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {str(e)}")
        # å‡ºé”™æ—¶é»˜è®¤ä¸é‡æ–°è§„åˆ’
        return {
            "replan": False,
            "next": "end",
            "plan": state.get("plan", []),  # ä¿ç•™æ‰§è¡Œè®¡åˆ’
            "messages": state.get("messages", [])  # ä¿ç•™å¯¹è¯å†å²
        }

# ====================================
# 5. Summarizerï¼ˆç”Ÿæˆæœ€ç»ˆå›å¤ï¼‰
# ====================================
async def summarizer(state: State, writer:StreamWriter):
    """ä½¿ç”¨LLMç”Ÿæˆæœ€ç»ˆå›å¤ - ä½¿ç”¨StreamWriterè¿›è¡Œæµå¼è¾“å‡º"""
    # è·å–æœ€æ–°çš„ç”¨æˆ·æŸ¥è¯¢ï¼ˆä»messagesä¸­æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼‰
    user_query = ""
    for msg in reversed(state["messages"]):
        # ä½¿ç”¨å±æ€§è®¿é—®è€Œä¸æ˜¯å­—å…¸è®¿é—®ï¼Œå› ä¸ºmsgæ˜¯Messageå¯¹è±¡
        if hasattr(msg, 'role') and msg.role == "user":
            user_query = msg.content if hasattr(msg, 'content') else ""
            break

    messages = state["messages"]
    # æå–å·¥å…·æ‰§è¡Œç»“æœï¼ˆä»…æœ¬æ¬¡å¯¹è¯ï¼‰
    recent_tool_messages = get_recent_run_tool_messages(messages, state["run_id"])
    tool_results = extract_tool_results(recent_tool_messages)
    if not tool_results:
        final_result = ""
    else:
        final_result = json.dumps(tool_results, ensure_ascii=False, indent=2)

    logger.info("\nSummarizer: ç”Ÿæˆæœ€ç»ˆå›å¤ï¼ˆä½¿ç”¨StreamWriteræµå¼è¾“å‡ºï¼‰")

    # åˆå§‹åŒ–ç´¯ç§¯å›å¤
    accumulated_reply = ""
    final_messages = state["messages"].copy()  
    # logger.info(f"Summarizer:å›å¤å‰messages - {final_messages}")
    conversation = "\n".join(
        f"{m.type}: {m.content}" for m in final_messages[:-1] 
        if m.type in ['human', 'ai'] and hasattr(m, 'content') and m.content.strip()  # åªåŒ…å«humanå’Œaiç±»å‹ä¸”å†…å®¹éç©º
    )
    logger.info(f"Summarizer: å¯¹è¯å†å² - {conversation}")
    # ä½¿ç”¨ä»YAMLåŠ è½½çš„æç¤ºè¯
    system_prompt = summarizer_prompt_data['system']
    
    # æ„å»ºå®Œæ•´çš„å¯¹è¯å†å²ä½œä¸ºä¸Šä¸‹æ–‡
    summary_prompt = [
        SystemMessage(content=system_prompt.format(
            user_query=user_query,
            final_result=final_result,
            conversation=conversation
        ))
    ]

    try:
        if hasattr(llm, "astream"):
            # å¦‚æœLLMæ”¯æŒå¼‚æ­¥æµå¼è°ƒç”¨
            async for chunk in llm.astream(summary_prompt):
                if hasattr(chunk, 'content') and chunk.content:
                    # æ›´æ–°ç´¯ç§¯å›å¤
                    accumulated_reply += chunk.content
                    
                    # ä½¿ç”¨StreamWriterå‘é€æµå¼æ•°æ®
                    writer({
                        "event_type": "custom_stream",
                        "messages": final_messages + [{"role": "assistant", "content": accumulated_reply}],
                        "reply": chunk.content,
                        "is_partial": True
                    })
    except Exception as e:
        logger.error(f"Summarizer: ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {str(e)}")
        # å‡ºé”™æ—¶ä½¿ç”¨æ‰§è¡Œç»“æœä½œä¸ºå›å¤
        accumulated_reply = final_result
        
        writer({
                "event_type": "custom_stream",
                "messages": final_messages + [{"role": "assistant", "content": accumulated_reply}],
                "reply": accumulated_reply,
                "is_partial": False
            })
    final_messages.append(AIMessage(content=accumulated_reply))
    logger.info(f"Summarizer: æœ€ç»ˆmessages - {final_messages}")
    # è¿”å›æœ€ç»ˆçŠ¶æ€ï¼Œè€Œä¸æ˜¯ä½¿ç”¨yield
    return {
        "messages": final_messages,
        "reply": accumulated_reply,
        "plan": state.get("plan", [])
    }

# ================================
# 8. æ„å»º LangGraph
# ================================

graph = StateGraph(State)

# æ·»åŠ èŠ‚ç‚¹
graph.add_node("planner", planner, aflow=True)  # Plannerç°åœ¨æ˜¯å¼‚æ­¥èŠ‚ç‚¹
graph.add_node("executor", executor)  # æ‰§è¡Œè®¡åˆ’ç®¡ç†æ­¥éª¤
# graph.add_node("router", router, aflow=True)     # æ‰§è¡Œå·¥å…·å¤„ç†è¾“å…¥è¾“å‡ºï¼ˆå¼‚æ­¥èŠ‚ç‚¹ï¼‰
# å‰é¢è·å–çš„mcpå·¥å…·
tools = asyncio.run(get_tools_for_toolnode())
tool_node = ToolNode(tools)

graph.add_node("tools", tool_node)
graph.add_node("checker", checker, aflow=True)   # æ£€æŸ¥ç»“æœæ˜¯å¦æ»¡è¶³è¦æ±‚ï¼ˆå¼‚æ­¥èŠ‚ç‚¹ï¼‰
graph.add_node("summarizer", summarizer, aflow=True)  # ç”Ÿæˆæœ€ç»ˆå›å¤ï¼ˆå¼‚æ­¥èŠ‚ç‚¹ï¼‰

# è®¾ç½®å…¥å£ç‚¹
graph.set_entry_point("planner")
# è¿æ¥èŠ‚ç‚¹
graph.add_edge("planner", "executor")  # Plannerç”Ÿæˆè®¡åˆ’åäº¤ç»™Executor

# æ·»åŠ æ¡ä»¶è¾¹ï¼Œæ ¹æ®executorè¿”å›çš„çŠ¶æ€å†³å®šä¸‹ä¸€æ­¥
graph.add_conditional_edges(
    "executor",
    lambda x: x.get("next", "end"),
    {
        "tools": "tools",            # ğŸ”‘ æ–°å¢ï¼šäº¤ç»™ ToolNode æ‰§è¡Œ
        "check": "checker",
        "summarizer": "summarizer",
        "end": END
    }
)
graph.add_edge("tools", "executor")
# æ£€æŸ¥å™¨çš„æ¡ä»¶è¾¹
graph.add_conditional_edges(
    "checker",
    # æ¡ä»¶å‡½æ•°ï¼šæ ¹æ®nextå­—æ®µå†³å®šä¸‹ä¸€ä¸ªèŠ‚ç‚¹
    lambda x: x.get("next", "end"),
    {
        "replan": "planner",   # éœ€è¦é‡æ–°è§„åˆ’ï¼Œè¿”å›planner
        "end": "summarizer"   # æ»¡è¶³éœ€æ±‚ï¼Œç”Ÿæˆæœ€ç»ˆå›å¤
    }
)
# graph_png = graph.get_graph().draw_mermaid_png()
# ç¼–è¯‘å›¾ï¼Œæ”¯æŒæµå¼è¾“å‡º
app_bindtools = graph.compile(name="bridge_bindtools")

logger.info("BridgeBindTools: å›¾ç¼–è¯‘å®Œæˆï¼Œæ”¯æŒæµå¼è¾“å‡º")

if __name__ == "__main__":
    print("=== LangGraph Agent å¯¹è¯ç³»ç»Ÿ ===")
    print("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–æŒ‡ä»¤ï¼Œè¾“å…¥'é€€å‡º'æˆ–'exit'ç»“æŸå¯¹è¯\n")
    
    state = None  # ç”¨äºç»´æŠ¤å¯¹è¯å†å²çŠ¶æ€
    
    # è¿›å…¥å¯¹è¯å¾ªç¯
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("ä½ : ")
            
            # æ£€æŸ¥æ˜¯å¦é€€å‡º
            if user_input.lower() in ['é€€å‡º', 'exit', 'quit', 'q']:
                print("æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break
            
            # è¿è¡Œagentå¤„ç†ç”¨æˆ·è¾“å…¥
            state = asyncio.run(run_agent(user_input, state))
            
        except KeyboardInterrupt:
            print("\nå¯¹è¯å·²ä¸­æ–­ï¼Œæ„Ÿè°¢ä½¿ç”¨ï¼")
            break
        except Exception as e:
            logger.error(f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}")
            print(f"å‘ç”Ÿé”™è¯¯: {str(e)}")